**Note:** This project has been migrated from Docker to PM2 process management. See `/root/PM2_MIGRATION_GUIDE.md` for details.



# 11. Observability, Metrics & Grafana Dashboards

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human developers.  
> **Goal of this file:** Add **production‑grade observability** to the AI chat platform:
>
> - Expose **Prometheus metrics** from the API gateway (`/metrics`).  
> - Instrument **HTTP requests** and **chat completions** (latency, tokens, errors).  
> - Provide a ready‑to‑run **Prometheus + Grafana** stack for local monitoring.  
> - Define a starter **Grafana dashboard** focused on AI chat performance and usage.
>
> After following this file, you will be able to open Grafana and see *live* charts of request latency, token usage, and model performance while chatting via the web UI.

> **Important instructions to AI agents:**
> - Do **not** modify database schemas here. Only add metrics and observability.  
> - Keep metrics names **stable and explicit** (Prometheus best practices).  
> - All code in this file must be **fully implemented**, without placeholders.

---

## 11.1. Metrics Design Overview

We focus on two layers of metrics:

1. **HTTP layer (API gateway)**
   - `http_requests_total{method,route,status_code}`  
   - `http_request_duration_seconds{method,route,status_code}`

2. **Chat completion layer**
   - `chat_completion_duration_seconds{model,streaming}` – end‑to‑end completion latency.
   - `chat_completion_tokens_total{model,type}` – token counts (`type` = `prompt` or `completion`).

All metrics will be exposed via a single endpoint:

- `GET /metrics` → Prometheus exposition format (text/plain).

---

## 11.2. Add `prom-client` Dependency to API Gateway

We use `prom-client` directly in the API gateway.

### 11.2.1. Update `apps/api-gateway/package.json`

Edit `apps/api-gateway/package.json` and ensure `prom-client` exists in `dependencies`:

```jsonc
{
  "name": "api-gateway",
  "version": "0.1.0",
  "private": true,
  "dependencies": {
    // ...existing deps...
    "prom-client": "^15.0.0"
  }
}
```

> **AI Agent Note:**  
> Merge this into the existing file without removing other dependencies such as `fastify`, `@ai-chat/config`, `@ai-chat/db`, `@ai-chat/chat-orchestrator`, etc.

From the repo root, later run:

```bash
pnpm install
```

---

## 11.3. Metrics Registry Module

We centralize metric definitions in a single module.

### 11.3.1. Create `apps/api-gateway/src/metrics.ts`

Create `apps/api-gateway/src/metrics.ts` with:

```ts
import { Counter, Histogram, Registry, collectDefaultMetrics } from 'prom-client';

// Dedicated registry for this service
const registry = new Registry();

// Collect Node.js default metrics (event loop, memory, etc.)
collectDefaultMetrics({ register: registry });

// HTTP request metrics
export const httpRequestsTotal = new Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code'] as const,
  registers: [registry]
});

export const httpRequestDurationSeconds = new Histogram({
  name: 'http_request_duration_seconds',
  help: 'HTTP request duration in seconds',
  labelNames: ['method', 'route', 'status_code'] as const,
  buckets: [0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10],
  registers: [registry]
});

// Chat completion metrics
export const chatCompletionDurationSeconds = new Histogram({
  name: 'chat_completion_duration_seconds',
  help: 'Chat completion duration in seconds for LLM calls',
  labelNames: ['model', 'streaming'] as const,
  buckets: [0.1, 0.25, 0.5, 1, 2, 5, 10, 20, 40],
  registers: [registry]
});

export const chatCompletionTokensTotal = new Counter({
  name: 'chat_completion_tokens_total',
  help: 'Total tokens used by chat completions',
  labelNames: ['model', 'type'] as const, // type: prompt|completion
  registers: [registry]
});

export async function getMetrics(): Promise<string> {
  return registry.metrics();
}

export function getMetricsContentType(): string {
  return registry.contentType;
}
```

This module:

- Sets up a dedicated registry.  
- Exposes counters/histograms for HTTP + chat.  
- Provides helpers (`getMetrics`, `getMetricsContentType`) for the `/metrics` route.

---

## 11.4. Fastify Metrics Plugin

We add a Fastify plugin to:

- Hook into **every request** to record latency and counts.  
- Expose `GET /metrics` using the registry above.

### 11.4.1. Create `apps/api-gateway/src/plugins/metrics.ts`

Create `apps/api-gateway/src/plugins/metrics.ts`:

```ts
import { FastifyPluginCallback } from 'fastify';
import { getMetrics, getMetricsContentType, httpRequestsTotal, httpRequestDurationSeconds } from '../metrics';

const metricsPlugin: FastifyPluginCallback = (app, _opts, done) => {
  const startTimeSymbol = Symbol('startTime');

  // Measure request start time
  app.addHook('onRequest', async (request) => {
    (request as any)[startTimeSymbol] = process.hrtime.bigint();
  });

  // Record metrics on response
  app.addHook('onResponse', async (request, reply) => {
    const startTime = (request as any)[startTimeSymbol] as bigint | undefined;

    const method = request.method;
    const rawUrl = request.routerPath || request.raw.url || '';
    const statusCode = reply.statusCode;

    const route = rawUrl || 'unknown';

    httpRequestsTotal.inc({
      method,
      route,
      status_code: String(statusCode)
    });

    if (startTime) {
      const endTime = process.hrtime.bigint();
      const diffNs = Number(endTime - startTime);
      const seconds = diffNs / 1e9;

      httpRequestDurationSeconds.observe(
        {
          method,
          route,
          status_code: String(statusCode)
        },
        seconds
      );
    }
  });

  // /metrics endpoint (no auth)
  app.get('/metrics', async (_request, reply) => {
    const metrics = await getMetrics();
    reply
      .header('Content-Type', getMetricsContentType())
      .send(metrics);
  });

  done();
};

export default metricsPlugin;
```

> **Note:** We intentionally keep `/metrics` **unauthenticated** for Prometheus scraping. In production you can restrict access via network policies or a reverse proxy.

---

## 11.5. Wire Metrics Plugin in `main.ts`

We now register the new `metricsPlugin` alongside existing plugins.

### 11.5.1. Update `apps/api-gateway/src/main.ts`

Replace the contents of `apps/api-gateway/src/main.ts` with the following, preserving all previous functionality and adding metrics:

```ts
import fastify from 'fastify';
import cors from 'fastify-cors';
import { getConfig } from '@ai-chat/config';
import { ensureDbExtensions, checkDbConnection } from '@ai-chat/db';
import authPlugin from './plugins/auth';
import metricsPlugin from './plugins/metrics';
import authRoutes from './routes/auth';
import orgRoutes from './routes/orgs';
import conversationsRoutes from './routes/conversations';
import chatRoutes from './routes/chat';

async function buildServer() {
  const config = getConfig();

  const app = fastify({
    logger: {
      level: config.LOG_LEVEL
    }
  });

  await app.register(cors, {
    origin: true,
    credentials: true
  });

  // Metrics plugin (adds /metrics and HTTP request metrics)
  await app.register(metricsPlugin);

  // Health check route (no auth)
  app.get('/health', async () => {
    const dbOk = await checkDbConnection();
    return {
      status: 'ok',
      env: config.NODE_ENV,
      db: dbOk ? 'up' : 'down',
      defaultModel: config.DEFAULT_MODEL
    };
  });

  // Auth plugin (JWT + authenticate hook)
  await app.register(authPlugin);

  // Routes
  await app.register(authRoutes);
  await app.register(orgRoutes);
  await app.register(conversationsRoutes);
  await app.register(chatRoutes);

  // Ensure DB extensions
  await ensureDbExtensions();

  return app;
}

async function start() {
  const config = getConfig();
  const port = config.API_PORT;
  const host = config.API_HOST;

  const app = await buildServer();

  app
    .listen({ port, host })
    .then(() => {
      app.log.info(`API Gateway listening on http://${host}:${port}`);
    })
    .catch((err) => {
      app.log.error(err, 'Failed to start API Gateway');
      process.exit(1);
    });
}

start();
```

At this point, once you restart the API gateway, hitting `http://localhost:4000/metrics` should return a Prometheus metrics text payload.

---

## 11.6. Instrument Chat Routes for Model Metrics

We now connect chat completion logic to the metrics module.

> **Important:** This section **replaces** the existing `apps/api-gateway/src/routes/chat.ts` file created in earlier markdowns. The behavior of the endpoints remains the same; we only add metrics.

### 11.6.1. Update `apps/api-gateway/src/routes/chat.ts`

Replace `apps/api-gateway/src/routes/chat.ts` with:

```ts
import { FastifyInstance, FastifyPluginOptions } from 'fastify';
import { prisma } from '@ai-chat/db';
import { JwtPayload } from '../auth/types';
import {
  ConversationContext,
  OrchestratorOptions,
  createUserMessage,
  runChatCompletion,
  streamChatCompletionOrchestrated
} from '@ai-chat/chat-orchestrator';
import { ChatMessage, ChatRole, ChatStreamEvent } from '@ai-chat/core-types';
import { z } from 'zod';
import {
  chatCompletionDurationSeconds,
  chatCompletionTokensTotal
} from '../metrics';

const sendMessageBodySchema = z.object({
  content: z.string().min(1),
  model: z.string().optional(),
  temperature: z.number().min(0).max(2).optional(),
  topP: z.number().min(0).max(1).optional(),
  maxTokens: z.number().int().positive().optional()
});

function mapDbRoleToChatRole(dbRole: string): ChatRole {
  switch (dbRole) {
    case 'SYSTEM':
      return 'system';
    case 'ASSISTANT':
      return 'assistant';
    case 'TOOL':
      return 'tool';
    case 'USER':
    default:
      return 'user';
  }
}

function buildConversationContext(conversation: any): ConversationContext {
  const history: ChatMessage[] = conversation.messages.map((m: any) => ({
    id: m.id,
    role: mapDbRoleToChatRole(m.role),
    content: m.content,
    createdAt: m.createdAt.toISOString()
  }));

  const ctx: ConversationContext = {
    id: conversation.id,
    title: conversation.title ?? undefined,
    systemPrompt: conversation.systemPrompt ?? undefined,
    customInstructions: undefined,
    history
  };

  return ctx;
}

export default async function chatRoutes(app: FastifyInstance, _opts: FastifyPluginOptions) {
  const orchestratorOptions: OrchestratorOptions = {
    maxContextTokens: 4000
  };

  // Non-streaming message send
  app.post('/conversations/:id/messages', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const paramsSchema = z.object({ id: z.string().min(1) });
    const parseParams = paramsSchema.safeParse(request.params);
    if (!parseParams.success) {
      return reply.code(400).send({ error: 'Invalid conversation id' });
    }
    const conversationId = parseParams.data.id;

    const parseBody = sendMessageBodySchema.safeParse(request.body);
    if (!parseBody.success) {
      return reply.code(400).send({ error: 'Invalid message data', details: parseBody.error.format() });
    }

    const { content, model, temperature, topP, maxTokens } = parseBody.data;

    // Load conversation + messages, ensuring access rights
    const memberships = await prisma.orgMember.findMany({
      where: { userId: payload.userId },
      select: { orgId: true }
    });
    const orgIds = memberships.map((m) => m.orgId);

    const orConditions: any[] = [{ userId: payload.userId }];
    if (orgIds.length > 0) {
      orConditions.push({ orgId: { in: orgIds } });
    }

    const conversation = await prisma.conversation.findFirst({
      where: {
        id: conversationId,
        OR: orConditions
      },
      include: {
        messages: {
          orderBy: { createdAt: 'asc' },
          take: 200
        }
      }
    });

    if (!conversation) {
      return reply.code(404).send({ error: 'Conversation not found' });
    }

    // Persist user message immediately
    const userMessageRecord = await prisma.message.create({
      data: {
        conversationId: conversation.id,
        role: 'USER',
        content,
        meta: {}
      }
    });

    const conversationWithNewMessage = {
      ...conversation,
      messages: [...conversation.messages, userMessageRecord]
    };

    const context = buildConversationContext(conversationWithNewMessage);
    const userMessage = createUserMessage(content);

    const chosenModel = model ?? conversation.model ?? 'llama3.1';

    const endTimer = chatCompletionDurationSeconds.startTimer({
      model: chosenModel,
      streaming: 'false'
    });

    const response = await runChatCompletion(
      context,
      userMessage,
      orchestratorOptions,
      {
        model: chosenModel,
        temperature: temperature ?? conversation.temperature ?? 0.7,
        topP: topP ?? conversation.topP ?? 1,
        maxTokens
      }
    );

    endTimer();

    if (response.usage) {
      const { promptTokens, completionTokens } = response.usage;
      if (typeof promptTokens === 'number') {
        chatCompletionTokensTotal.inc({ model: chosenModel, type: 'prompt' }, promptTokens);
      }
      if (typeof completionTokens === 'number') {
        chatCompletionTokensTotal.inc({ model: chosenModel, type: 'completion' }, completionTokens);
      }
    }

    // Persist assistant message
    const assistantMessageRecord = await prisma.message.create({
      data: {
        conversationId: conversation.id,
        role: 'ASSISTANT',
        content: response.message.content,
        meta: {
          usage: response.usage ?? null,
          providerMeta: response.providerMeta ?? null
        }
      }
    });

    await prisma.conversation.update({
      where: { id: conversation.id },
      data: {
        updatedAt: new Date()
      }
    });

    return reply.send({
      conversationId: conversation.id,
      userMessage: {
        id: userMessageRecord.id,
        role: userMessageRecord.role,
        content: userMessageRecord.content,
        createdAt: userMessageRecord.createdAt
      },
      assistantMessage: {
        id: assistantMessageRecord.id,
        role: assistantMessageRecord.role,
        content: assistantMessageRecord.content,
        createdAt: assistantMessageRecord.createdAt
      },
      usage: response.usage
    });
  });

  // Streaming message send via SSE
  app.post('/conversations/:id/stream', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const paramsSchema = z.object({ id: z.string().min(1) });
    const parseParams = paramsSchema.safeParse(request.params);
    if (!parseParams.success) {
      return reply.code(400).send({ error: 'Invalid conversation id' });
    }
    const conversationId = parseParams.data.id;

    const parseBody = sendMessageBodySchema.safeParse(request.body);
    if (!parseBody.success) {
      return reply.code(400).send({ error: 'Invalid message data', details: parseBody.error.format() });
    }

    const { content, model, temperature, topP, maxTokens } = parseBody.data;

    // Load conversation + messages, ensuring access rights
    const memberships = await prisma.orgMember.findMany({
      where: { userId: payload.userId },
      select: { orgId: true }
    });
    const orgIds = memberships.map((m) => m.orgId);

    const orConditions: any[] = [{ userId: payload.userId }];
    if (orgIds.length > 0) {
      orConditions.push({ orgId: { in: orgIds } });
    }

    const conversation = await prisma.conversation.findFirst({
      where: {
        id: conversationId,
        OR: orConditions
      },
      include: {
        messages: {
          orderBy: { createdAt: 'asc' },
          take: 200
        }
      }
    });

    if (!conversation) {
      return reply.code(404).send({ error: 'Conversation not found' });
    }

    // Persist user message immediately
    const userMessageRecord = await prisma.message.create({
      data: {
        conversationId: conversation.id,
        role: 'USER',
        content,
        meta: {}
      }
    });

    const conversationWithNewMessage = {
      ...conversation,
      messages: [...conversation.messages, userMessageRecord]
    };

    const context = buildConversationContext(conversationWithNewMessage);
    const userMessage = createUserMessage(content);

    const chosenModel = model ?? conversation.model ?? 'llama3.1';

    const endTimer = chatCompletionDurationSeconds.startTimer({
      model: chosenModel,
      streaming: 'true'
    });

    // Set up SSE headers
    reply.raw.setHeader('Content-Type', 'text/event-stream');
    reply.raw.setHeader('Cache-Control', 'no-cache');
    reply.raw.setHeader('Connection', 'keep-alive');
    reply.raw.flushHeaders?.();

    const sendEvent = (event: unknown) => {
      reply.raw.write(`data: ${JSON.stringify(event)}\n\n`);
    };

    const abortController = new AbortController();

    request.raw.on('close', () => {
      abortController.abort();
    });

    let finalAssistantContent = '';
    let finalUsage: any = null;

    try {
      for await (const event of streamChatCompletionOrchestrated(
        context,
        userMessage,
        orchestratorOptions,
        {
          model: chosenModel,
          temperature: temperature ?? conversation.temperature ?? 0.7,
          topP: topP ?? conversation.topP ?? 1,
          maxTokens,
          signal: abortController.signal
        }
      )) {
        const outgoing: any = { type: event.type };

        if (event.type === 'token' && event.token) {
          outgoing.token = event.token;
          finalAssistantContent += event.token;
        }

        if (event.type === 'end') {
          if (event.finalMessage) {
            finalAssistantContent = event.finalMessage.content;
          }
          outgoing.message = {
            role: 'assistant',
            content: finalAssistantContent
          };
          if (event.usage) {
            outgoing.usage = event.usage;
            finalUsage = event.usage;
          }
        }

        if (event.type === 'error' && event.error) {
          outgoing.error = event.error;
        }

        sendEvent(outgoing as ChatStreamEvent);
      }

      endTimer();

      if (finalUsage) {
        const { promptTokens, completionTokens } = finalUsage;
        if (typeof promptTokens === 'number') {
          chatCompletionTokensTotal.inc({ model: chosenModel, type: 'prompt' }, promptTokens);
        }
        if (typeof completionTokens === 'number') {
          chatCompletionTokensTotal.inc({ model: chosenModel, type: 'completion' }, completionTokens);
        }
      }

      // Persist assistant message if we have any content
      if (finalAssistantContent.length > 0) {
        await prisma.message.create({
          data: {
            conversationId: conversation.id,
            role: 'ASSISTANT',
            content: finalAssistantContent,
            meta: {
              usage: finalUsage,
              providerMeta: null
            }
          }
        });

        await prisma.conversation.update({
          where: { id: conversation.id },
          data: {
            updatedAt: new Date()
          }
        });
      }

      reply.raw.end();
    } catch (err) {
      endTimer();

      try {
        sendEvent({ type: 'error', error: (err as Error).message });
        reply.raw.end();
      } catch {
        // Connection already closed
      }
    }
  });
}
```

With this, every completion (streaming or non‑streaming) is recorded with latency and token metrics.

---

## 11.7. Local Prometheus + Grafana Stack

We now provide a simple Docker‑based monitoring stack.

### 11.7.1. Directory Structure

Create:

```text
infra/monitoring/
  docker-compose.yml
  prometheus.yml
```

### 11.7.2. `infra/monitoring/prometheus.yml`

Create `infra/monitoring/prometheus.yml`:

```yaml
global:
  scrape_interval: 5s

scrape_configs:
  - job_name: 'api-gateway'
    static_configs:
      - targets:
          - 'host.docker.internal:4000'
    metrics_path: /metrics
```

This configuration tells Prometheus to scrape the API gateway metrics from the host at port `4000`.

### 11.7.3. `infra/monitoring/docker-compose.yml`

Create `infra/monitoring/docker-compose.yml`:

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: ai-chat-prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - '9090:9090'

  grafana:
    image: grafana/grafana-oss:latest
    container_name: ai-chat-grafana
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus
```

> **Note:** This stack expects your API gateway to be reachable at `http://localhost:4000`. On macOS and Windows, `host.docker.internal` resolves correctly from inside Docker. On Linux, use your host IP instead.

### 11.7.4. Running the Monitoring Stack

From the repo root:

```bash
cd infra/monitoring
docker compose up -d
```

Then:

- Prometheus UI: `http://localhost:9090`  
- Grafana UI: `http://localhost:3000`  (login: `admin` / `admin`)

In Grafana:

1. Add a new **Prometheus** data source with URL `http://prometheus:9090`.  
2. Save & test.

---

## 11.8. Starter Grafana Dashboard

Below is a minimal dashboard layout you can configure through Grafana’s UI using the metrics we exposed.

### 11.8.1. Suggested Panels

1. **Panel: API Gateway – Request Rate**
   - Type: Time series  
   - Query:
     ```promql
     sum by (route, method) (rate(http_requests_total[1m]))
     ```
   - Legend: `{{method}} {{route}}`

2. **Panel: API Gateway – Request Latency (P95)**
   - Type: Time series  
   - Query:
     ```promql
     histogram_quantile(0.95, sum by (le, route) (rate(http_request_duration_seconds_bucket[5m])))
     ```
   - Legend: `p95 {{route}}`

3. **Panel: Chat – Completion Latency by Model**
   - Type: Time series  
   - Query:
     ```promql
     histogram_quantile(0.95, sum by (le, model) (rate(chat_completion_duration_seconds_bucket[5m])))
     ```
   - Legend: `{{model}}`

4. **Panel: Chat – Prompt Tokens per Second**
   - Type: Time series  
   - Query:
     ```promql
     sum by (model) (rate(chat_completion_tokens_total{type="prompt"}[1m]))
     ```
   - Legend: `prompt {{model}}`

5. **Panel: Chat – Completion Tokens per Second**
   - Type: Time series  
   - Query:
     ```promql
     sum by (model) (rate(chat_completion_tokens_total{type="completion"}[1m]))
     ```
   - Legend: `completion {{model}}`

These panels give you an immediate view into:

- Traffic hotspots by route.  
- Latency behavior under load.  
- Model‑level token usage and throughput.

---

## 11.9. Sanity Checks

From the repo root:

1. **Install dependencies:**

   ```bash
   pnpm install
   ```

2. **Restart API gateway:**

   ```bash
   pnpm dev --filter=api-gateway
   ```

3. **Verify metrics endpoint:**

   ```bash
   curl http://localhost:4000/metrics | head -40
   ```

   You should see:

   - `http_requests_total`  
   - `http_request_duration_seconds`  
   - `chat_completion_duration_seconds`  
   - `chat_completion_tokens_total`  
   - plus default Node metrics.

4. **Start monitoring stack:**

   ```bash
   cd infra/monitoring
   docker compose up -d
   ```

5. **Generate load:**

   - Use the web UI to send several chats.  
   - In Grafana, watch your dashboard panels; they should react in real time.

If all of the above works, your AI chat platform now has **industry‑grade observability** with Prometheus + Grafana.

---

## 11.10. Next Steps

Future markdown files can extend observability by:

- Emitting **per‑tenant/org usage metrics** (for billing and quotas).  
- Adding **error rate panels** and alerts (e.g., high `5xx` or provider errors).  
- Surfacing key metrics directly into the web UI (usage chips, per‑conversation token stats).

> **AI Agent Instruction:**  
> Before moving on, ensure that:
> - `http://localhost:4000/metrics` returns metrics.  
> - The Docker monitoring stack is running.  
> - Grafana panels show live data when you chat through the web UI.

---

_End of 11.md – Observability, Metrics & Grafana Dashboards_
