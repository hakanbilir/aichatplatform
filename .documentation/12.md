# 12. Per‑Conversation Settings – Model, Temperature & UI Controls

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human developers.  
> **Goal of this file:** Add **per‑conversation settings** that feel like a modern ChatGPT‑style control bar:
>
> - Backend: `PATCH /conversations/:id` to update `title`, `model`, `temperature`, `topP`, `systemPrompt`.  
> - Frontend API client: `updateConversation(...)`.  
> - Frontend UI: Material‑3‑style mini settings bar above the chat with:
>   - **Model selector** (e.g. `llama3.1`, `llama3.1:8b`, `qwen2.5-coder`).  
>   - **Temperature slider** (0–2) with subtle micro‑interactions.  
>   - **Top‑p slider** (0–1).  
>   - **Auto‑save** button and small “Saved” indicator.
>
> After following this file, each conversation can have its own **model + generation settings**, stored in the DB and applied to both streaming and non‑streaming calls.

> **Important instructions to AI agents:**
> - This file **extends** the existing implementation from `7.md`, `9.md`, `10.md`, `11.md`.  
> - When we say “replace file X”, treat the new content as canonical.  
> - Do not leave any TODOs; every function must be fully implemented.

---

## 12.1. Backend: Extend Conversation Routes with PATCH

We add a `PATCH /conversations/:id` route to update conversation‑level settings.  
We assume the Prisma `Conversation` model already has fields:

- `title?: string | null`  
- `model?: string | null`  
- `systemPrompt?: string | null`  
- `temperature?: number | null`  
- `topP?: number | null`

### 12.1.1. Replace `apps/api-gateway/src/routes/conversations.ts`

Replace the entire file `apps/api-gateway/src/routes/conversations.ts` with this implementation (includes list, create, get, and patch):

```ts
import { FastifyInstance, FastifyPluginOptions } from 'fastify';
import { prisma } from '@ai-chat/db';
import { JwtPayload } from '../auth/types';
import { z } from 'zod';

const createConversationBodySchema = z.object({
  title: z.string().min(1).max(200).optional(),
  systemPrompt: z.string().max(8000).optional(),
  model: z.string().max(200).optional(),
  temperature: z.number().min(0).max(2).optional(),
  topP: z.number().min(0).max(1).optional()
});

const updateConversationBodySchema = z.object({
  title: z.string().min(1).max(200).optional(),
  systemPrompt: z.string().max(8000).nullable().optional(),
  model: z.string().max(200).nullable().optional(),
  temperature: z.number().min(0).max(2).nullable().optional(),
  topP: z.number().min(0).max(1).nullable().optional()
});

function getUserOrgIds(userId: string) {
  return prisma.orgMember
    .findMany({
      where: { userId },
      select: { orgId: true }
    })
    .then((rows) => rows.map((r) => r.orgId));
}

export default async function conversationsRoutes(app: FastifyInstance, _opts: FastifyPluginOptions) {
  // List conversations visible to the user (own + orgs)
  app.get('/conversations', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const orgIds = await getUserOrgIds(payload.userId);
    const orConditions: any[] = [{ userId: payload.userId }];
    if (orgIds.length > 0) {
      orConditions.push({ orgId: { in: orgIds } });
    }

    const conversations = await prisma.conversation.findMany({
      where: {
        OR: orConditions
      },
      orderBy: {
        updatedAt: 'desc'
      },
      take: 100
    });

    return reply.send({
      conversations: conversations.map((c) => ({
        id: c.id,
        title: c.title,
        model: c.model,
        createdAt: c.createdAt,
        updatedAt: c.updatedAt,
        orgId: c.orgId
      }))
    });
  });

  // Create conversation
  app.post('/conversations', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const parseBody = createConversationBodySchema.safeParse(request.body);
    if (!parseBody.success) {
      return reply.code(400).send({ error: 'Invalid conversation data', details: parseBody.error.format() });
    }

    const { title, systemPrompt, model, temperature, topP } = parseBody.data;

    const conversation = await prisma.conversation.create({
      data: {
        title: title ?? null,
        systemPrompt: systemPrompt ?? null,
        model: model ?? null,
        temperature: temperature ?? null,
        topP: topP ?? null,
        userId: payload.userId,
        orgId: null
      }
    });

    return reply.code(201).send({
      id: conversation.id,
      title: conversation.title,
      model: conversation.model,
      createdAt: conversation.createdAt,
      updatedAt: conversation.updatedAt,
      orgId: conversation.orgId
    });
  });

  // Get conversation with messages
  app.get('/conversations/:id', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const paramsSchema = z.object({ id: z.string().min(1) });
    const parseParams = paramsSchema.safeParse(request.params);
    if (!parseParams.success) {
      return reply.code(400).send({ error: 'Invalid conversation id' });
    }

    const conversationId = parseParams.data.id;

    const orgIds = await getUserOrgIds(payload.userId);
    const orConditions: any[] = [{ userId: payload.userId }];
    if (orgIds.length > 0) {
      orConditions.push({ orgId: { in: orgIds } });
    }

    const conversation = await prisma.conversation.findFirst({
      where: {
        id: conversationId,
        OR: orConditions
      },
      include: {
        messages: {
          orderBy: { createdAt: 'asc' },
          take: 200
        }
      }
    });

    if (!conversation) {
      return reply.code(404).send({ error: 'Conversation not found' });
    }

    return reply.send({
      conversation: {
        id: conversation.id,
        title: conversation.title,
        model: conversation.model,
        systemPrompt: conversation.systemPrompt,
        temperature: conversation.temperature,
        topP: conversation.topP,
        createdAt: conversation.createdAt,
        updatedAt: conversation.updatedAt,
        orgId: conversation.orgId,
        messages: conversation.messages.map((m) => ({
          id: m.id,
          role: m.role,
          content: m.content,
          createdAt: m.createdAt
        }))
      }
    });
  });

  // Patch conversation settings (title/model/systemPrompt/temperature/topP)
  app.patch('/conversations/:id', { preHandler: [app.authenticate] }, async (request, reply) => {
    const payload = request.user as JwtPayload;

    const paramsSchema = z.object({ id: z.string().min(1) });
    const parseParams = paramsSchema.safeParse(request.params);
    if (!parseParams.success) {
      return reply.code(400).send({ error: 'Invalid conversation id' });
    }
    const conversationId = parseParams.data.id;

    const parseBody = updateConversationBodySchema.safeParse(request.body);
    if (!parseBody.success) {
      return reply.code(400).send({ error: 'Invalid conversation update', details: parseBody.error.format() });
    }

    const orgIds = await getUserOrgIds(payload.userId);
    const orConditions: any[] = [{ userId: payload.userId }];
    if (orgIds.length > 0) {
      orConditions.push({ orgId: { in: orgIds } });
    }

    const existing = await prisma.conversation.findFirst({
      where: {
        id: conversationId,
        OR: orConditions
      }
    });

    if (!existing) {
      return reply.code(404).send({ error: 'Conversation not found' });
    }

    const data: any = {};

    if ('title' in parseBody.data) {
      data.title = parseBody.data.title ?? null;
    }
    if ('systemPrompt' in parseBody.data) {
      data.systemPrompt = parseBody.data.systemPrompt ?? null;
    }
    if ('model' in parseBody.data) {
      data.model = parseBody.data.model ?? null;
    }
    if ('temperature' in parseBody.data) {
      data.temperature = parseBody.data.temperature ?? null;
    }
    if ('topP' in parseBody.data) {
      data.topP = parseBody.data.topP ?? null;
    }

    const updated = await prisma.conversation.update({
      where: { id: conversationId },
      data: {
        ...data,
        updatedAt: new Date()
      }
    });

    return reply.send({
      conversation: {
        id: updated.id,
        title: updated.title,
        model: updated.model,
        systemPrompt: updated.systemPrompt,
        temperature: updated.temperature,
        topP: updated.topP,
        createdAt: updated.createdAt,
        updatedAt: updated.updatedAt,
        orgId: updated.orgId
      }
    });
  });
}
```

This route is used by the frontend settings UI to persist user choices per conversation.

---

## 12.2. Frontend API: `updateConversation`

We now extend the frontend conversations API module to support PATCH.

### 12.2.1. Replace `apps/web/src/api/conversations.ts`

Replace the entire `apps/web/src/api/conversations.ts` file with this version (includes the previous functions plus `updateConversation`):

```ts
import { apiRequest } from './client';

export interface ConversationListItem {
  id: string;
  title: string | null;
  model: string | null;
  createdAt: string;
  updatedAt: string;
  orgId: string | null;
}

export interface ConversationMessage {
  id: string;
  role: string;
  content: string;
  createdAt: string;
}

export interface ConversationDetails {
  id: string;
  title: string | null;
  model: string | null;
  systemPrompt: string | null;
  temperature: number | null;
  topP: number | null;
  createdAt: string;
  updatedAt: string;
  orgId: string | null;
  messages: ConversationMessage[];
}

export async function listConversations(token: string): Promise<{ conversations: ConversationListItem[] }> {
  return apiRequest<{ conversations: ConversationListItem[] }>('/conversations', { method: 'GET' }, token);
}

export async function createConversation(
  token: string,
  data: { title?: string; systemPrompt?: string; model?: string; temperature?: number; topP?: number }
): Promise<ConversationListItem> {
  const result = await apiRequest<ConversationListItem>(
    '/conversations',
    {
      method: 'POST',
      body: JSON.stringify(data)
    },
    token
  );
  return result;
}

export async function getConversation(token: string, id: string): Promise<{ conversation: ConversationDetails }> {
  return apiRequest<{ conversation: ConversationDetails }>(`/conversations/${id}`, { method: 'GET' }, token);
}

export interface UpdateConversationPayload {
  title?: string | null;
  systemPrompt?: string | null;
  model?: string | null;
  temperature?: number | null;
  topP?: number | null;
}

export async function updateConversation(
  token: string,
  id: string,
  data: UpdateConversationPayload
): Promise<{ conversation: ConversationDetails }> {
  return apiRequest<{ conversation: ConversationDetails }>(
    `/conversations/${id}`,
    {
      method: 'PATCH',
      body: JSON.stringify(data)
    },
    token
  );
}
```

---

## 12.3. Frontend UI: Conversation Settings Bar

We add a compact **settings bar** at the top of the chat area:

- Left: conversation title (for now, read‑only).  
- Center: **Model select**.  
- Right: **Temperature & top‑p sliders** and a tiny “Saved” indicator.

This is implemented inside `ChatPage` so it stays scoped to the selected conversation.

### 12.3.1. Replace `apps/web/src/chat/ChatPage.tsx`

Replace the entire `apps/web/src/chat/ChatPage.tsx` with the following implementation that wires settings into streaming:

```tsx
import React, { useEffect, useState, useRef } from 'react';
import { Box, Typography, FormControl, InputLabel, Select, MenuItem, Slider, Chip, IconButton, Tooltip } from '@mui/material';
import SaveIcon from '@mui/icons-material/Save';
import RestartAltIcon from '@mui/icons-material/RestartAlt';
import { useAuth } from '../auth/AuthContext';
import { ConversationDetails, getConversation, updateConversation } from '../api/conversations';
import { streamMessage, StreamEvent } from '../api/chat';
import { ChatView } from './ChatView';
import { MessageInput } from './MessageInput';

const MODEL_OPTIONS: { value: string; label: string }[] = [
  { value: 'llama3.1', label: 'llama3.1 (general)' },
  { value: 'llama3.1:8b', label: 'llama3.1:8b (fast)' },
  { value: 'qwen2.5-coder', label: 'qwen2.5-coder (code)' }
];

function clampTemperature(value: number): number {
  if (Number.isNaN(value)) return 0.7;
  if (value < 0) return 0;
  if (value > 2) return 2;
  return value;
}

function clampTopP(value: number): number {
  if (Number.isNaN(value)) return 1;
  if (value < 0) return 0;
  if (value > 1) return 1;
  return value;
}

export const ChatPage: React.FC = () => {
  const { token } = useAuth();
  const [conversationId, setConversationId] = useState<string | null>(null);
  const [conversation, setConversation] = useState<ConversationDetails | null>(null);
  const [streamingText, setStreamingText] = useState('');
  const [streaming, setStreaming] = useState(false);

  const [model, setModel] = useState<string>('llama3.1');
  const [temperature, setTemperature] = useState<number>(0.7);
  const [topP, setTopP] = useState<number>(1);
  const [dirty, setDirty] = useState<boolean>(false);
  const [saving, setSaving] = useState<boolean>(false);
  const [savedAt, setSavedAt] = useState<number | null>(null);

  const abortRef = useRef<AbortController | null>(null);

  // Listen for conversation selection/creation events from sidebar
  useEffect(() => {
    const handleSelect = (e: Event) => {
      const id = (e as CustomEvent<string>).detail;
      setConversationId(id);
    };

    const handleCreated = (e: Event) => {
      const id = (e as CustomEvent<string>).detail;
      setConversationId(id);
    };

    window.addEventListener('select-conversation', handleSelect);
    window.addEventListener('conversation-created', handleCreated);

    return () => {
      window.removeEventListener('select-conversation', handleSelect);
      window.removeEventListener('conversation-created', handleCreated);
    };
  }, []);

  // Load conversation details when id or auth changes
  useEffect(() => {
    if (!token || !conversationId) {
      setConversation(null);
      return;
    }

    let cancelled = false;

    async function load() {
      try {
        const resp = await getConversation(token, conversationId);
        if (!cancelled) {
          const conv = resp.conversation;
          setConversation(conv);
          const nextModel = conv.model || 'llama3.1';
          const nextTemp = clampTemperature(conv.temperature ?? 0.7);
          const nextTopP = clampTopP(conv.topP ?? 1);
          setModel(nextModel);
          setTemperature(nextTemp);
          setTopP(nextTopP);
          setStreamingText('');
          setDirty(false);
        }
      } catch {
        if (!cancelled) setConversation(null);
      }
    }

    load();

    return () => {
      cancelled = true;
    };
  }, [token, conversationId]);

  // Auto-clear the "Saved" chip after some time
  useEffect(() => {
    if (!savedAt) return;
    const timeout = window.setTimeout(() => {
      setSavedAt(null);
    }, 2000);
    return () => window.clearTimeout(timeout);
  }, [savedAt]);

  const handleSend = async (content: string) => {
    if (!token || !conversationId) return;

    if (!conversation) {
      return;
    }

    if (abortRef.current) {
      abortRef.current.abort();
    }

    const localConversationId = conversationId;

    // Optimistically append user message locally
    const userMessage = {
      id: `local-${Date.now()}`,
      role: 'USER',
      content
    };

    setConversation((prev) =>
      prev
        ? {
            ...prev,
            messages: [...prev.messages, { ...userMessage, createdAt: new Date().toISOString() }]
          }
        : prev
    );

    setStreamingText('');
    setStreaming(true);

    const controller = new AbortController();
    abortRef.current = controller;

    try {
      await streamMessage(
        token,
        localConversationId,
        {
          content,
          model,
          temperature,
          topP
        },
        (event: StreamEvent) => {
          if (event.type === 'token') {
            setStreamingText((prev) => prev + event.token);
          }

          if (event.type === 'end' && event.message) {
            setStreamingText('');
            setConversation((prev) =>
              prev
                ? {
                    ...prev,
                    messages: [
                      ...prev.messages,
                      {
                        id: `assistant-${Date.now()}`,
                        role: 'ASSISTANT',
                        content: event.message.content,
                        createdAt: new Date().toISOString()
                      }
                    ]
                  }
                : prev
            );
          }
        },
        controller.signal
      );
    } finally {
      setStreaming(false);
    }

    // Refresh from backend to align IDs and usage
    if (token && localConversationId) {
      try {
        const resp = await getConversation(token, localConversationId);
        setConversation(resp.conversation);
      } catch {
        // ignore
      }
    }
  };

  const handleSaveSettings = async () => {
    if (!token || !conversationId) return;

    setSaving(true);
    try {
      const resp = await updateConversation(token, conversationId, {
        model,
        temperature,
        topP
      });
      setConversation((prev) =>
        prev
          ? {
              ...prev,
              model: resp.conversation.model,
              temperature: resp.conversation.temperature,
              topP: resp.conversation.topP
            }
          : prev
      );
      setDirty(false);
      setSavedAt(Date.now());
    } finally {
      setSaving(false);
    }
  };

  const handleResetSettings = () => {
    if (!conversation) return;
    const baseModel = conversation.model || 'llama3.1';
    const baseTemp = clampTemperature(conversation.temperature ?? 0.7);
    const baseTopP = clampTopP(conversation.topP ?? 1);
    setModel(baseModel);
    setTemperature(baseTemp);
    setTopP(baseTopP);
    setDirty(false);
  };

  const handleChangeModel = (value: string) => {
    setModel(value);
    setDirty(true);
  };

  const handleChangeTemperature = (_: Event, value: number | number[]) => {
    const v = Array.isArray(value) ? value[0] : value;
    setTemperature(v);
    setDirty(true);
  };

  const handleChangeTopP = (_: Event, value: number | number[]) => {
    const v = Array.isArray(value) ? value[0] : value;
    setTopP(v);
    setDirty(true);
  };

  const currentTitle = conversation?.title || 'New conversation';

  const creativityLabel = temperature < 0.4 ? 'Precise' : temperature < 1 ? 'Balanced' : 'Creative';

  return (
    <Box display="flex" flexDirection="column" flex={1}>
      {/* Settings bar */}
      <Box
        px={2}
        py={1}
        sx={{
          display: 'flex',
          alignItems: 'center',
          gap: 2,
          borderBottom: '1px solid rgba(255,255,255,0.08)',
          background: 'radial-gradient(circle at top left, rgba(124,77,255,0.16), transparent 55%)'
        }}
      >
        <Box flex={1} minWidth={0}>
          <Typography variant="subtitle2" noWrap>
            {currentTitle}
          </Typography>
          <Typography variant="caption" color="text.secondary">
            Model &amp; creativity are per‑conversation.
          </Typography>
        </Box>

        <FormControl size="small" sx={{ minWidth: 210 }}>
          <InputLabel id="model-select-label">Model</InputLabel>
          <Select
            labelId="model-select-label"
            label="Model"
            value={model}
            onChange={(e) => handleChangeModel(e.target.value)}
          >
            {MODEL_OPTIONS.map((opt) => (
              <MenuItem key={opt.value} value={opt.value}>
                {opt.label}
              </MenuItem>
            ))}
          </Select>
        </FormControl>

        <Box width={160} px={1}>
          <Typography variant="caption" color="text.secondary">
            Temperature
          </Typography>
          <Slider
            size="small"
            value={temperature}
            min={0}
            max={2}
            step={0.1}
            onChange={handleChangeTemperature}
          />
        </Box>

        <Box width={130} px={1}>
          <Typography variant="caption" color="text.secondary">
            Top‑p
          </Typography>
          <Slider size="small" value={topP} min={0} max={1} step={0.05} onChange={handleChangeTopP} />
        </Box>

        <Chip
          size="small"
          label={creativityLabel}
          sx={{ fontSize: 11, height: 24 }}
          variant="outlined"
        />

        <Box display="flex" alignItems="center" gap={1}>
          <Tooltip title="Reset to saved settings">
            <span>
              <IconButton size="small" onClick={handleResetSettings} disabled={!conversation}>
                <RestartAltIcon fontSize="small" />
              </IconButton>
            </span>
          </Tooltip>
          <Tooltip title={dirty ? 'Save settings for this conversation' : 'Settings are up to date'}>
            <span>
              <IconButton
                size="small"
                color={dirty ? 'primary' : 'default'}
                onClick={handleSaveSettings}
                disabled={!dirty || saving || !conversationId}
              >
                <SaveIcon fontSize="small" />
              </IconButton>
            </span>
          </Tooltip>
          {savedAt && !dirty && (
            <Chip
              size="small"
              label="Saved"
              color="success"
              variant="outlined"
              sx={{ height: 22, fontSize: 11 }}
              className="micro-fade-in"
            />
          )}
        </Box>
      </Box>

      {/* Chat view + input */}
      <ChatView messages={conversation?.messages ?? []} streamingAssistantText={streamingText} />
      <MessageInput disabled={!conversationId || streaming} onSend={handleSend} />
    </Box>
  );
};
```

This component now:

- Keeps `model`, `temperature`, `topP` in local state.  
- Syncs them from the conversation when loaded.  
- Saves them to the backend via `updateConversation`.  
- Passes them into `streamMessage` so the backend actually uses the chosen settings.

---

## 12.4. Sidebar Integration (No Changes Required)

The sidebar already emits:

- `select-conversation` events when the user picks a conversation.  
- `conversation-created` events when a new chat is created.

`ChatPage` listens to these events and loads the conversation, so no sidebar changes are required for the settings bar to work.

---

## 12.5. Sanity Checks

From the repo root:

1. **Install dependencies (if not already):**

   ```bash
   pnpm install
   ```

2. **Run API gateway:**

   ```bash
   pnpm dev --filter=api-gateway
   ```

3. **Run web app:**

   ```bash
   cd apps/web
   pnpm dev
   ```

4. **Verify behavior:**

   - Open `http://localhost:5173`.  
   - Sign in and go to `/app`.  
   - Create or select a conversation.  
   - Use the **Model** select and sliders to change settings.  
   - Click the **save** icon → “Saved” chip should appear briefly.  
   - Send messages and observe that the backend now receives the selected `model`, `temperature` and `topP` in the streaming request body.

If all checks pass, your AI chat platform now supports **per‑conversation model and generation controls**, similar to modern ChatGPT‑style UIs.

---

## 12.6. Next Steps

Future markdown files can extend this further by:

- Allowing inline **title editing** from the settings bar.  
- Exposing **system prompt** editor UI (advanced mode).  
- Surfacing **per‑conversation usage metrics** from Prometheus (e.g., total tokens by conversation) into small chips.

> **AI Agent Instruction:**  
> Before moving on, ensure that `PATCH /conversations/:id` works end‑to‑end and that the web UI’s settings bar correctly saves and restores model/temperature/top‑p per conversation.

---

_End of 12.md – Per‑Conversation Settings – Model, Temperature & UI Controls_
