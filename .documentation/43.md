# 43. Model Registry, Provider Routing & Ollama Integration – Backend + Material 3 UI

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human backend/frontend engineers.  
> **Goal of this file:** Implement an **enterprise‑grade model layer** so that:
>
> - The platform supports **multiple LLM providers** (Ollama, OpenAI, Anthropic, etc.) behind a single abstraction.  
> - Orgs can **see and control** which models are available to them.  
> - Chat Profiles (42.md) can declaratively choose `modelProvider` + `modelName`.  
> - Ollama is treated as a **first‑class provider** (local models, streaming, multi‑tenant friendly).  
> - All calls are **observable** (28.md) and **auditable** (38.md).
>
> This spec is written so Cursor‑like agents can implement it end‑to‑end without further questions.

This file builds on:

- 11–14.md – Users, orgs, roles & RBAC.  
- 19.md – Quotas, rate limits & usage accounting.  
- 25.md – Conversation & message pipeline.  
- 33–37.md – Tools, RAG & function calling.  
- 38.md – Events & audit log.  
- 41.md – Safety & moderation hooks.  
- 42.md – ChatProfiles (`modelProvider`, `modelName`, `providerConfig`).

---

## 43.1. Concepts & Scope

We define the **model layer** with three main concepts:

1. **Model Registry Entry**  
   - Declarative definition of a model (provider, name, capabilities, limits, pricing) that can be used across the platform.  
   - May be **global** (platform‑wide) or **org‑scoped**.

2. **Provider Routing**  
   - Abstraction over provider specifics (Ollama HTTP API, OpenAI/Anthropic SDKs, etc.).  
   - Chat pipeline (25.md) never calls providers directly; it calls the **router** with a `ChatProfile` and messages.

3. **Ollama Integration**  
   - Treat **Ollama** as a production‑ready provider:  
     - Base URL config, health checks.  
     - Chat completion (streaming & non‑streaming) via `/api/chat`.  
     - Model listing via `/api/tags`.  
   - Exposed in Model Registry & UI for admins.

All of this must be:

- Multi‑tenant (`orgId` scoped where relevant).  
- RBAC‑aware (`org:models:*` permissions).  
- Integrated with **metrics**, **events** and **quotas**.

---

## 43.2. Data Model – Prisma

Extend `packages/db/prisma/schema.prisma` with a **Model Registry** and optional **Org Provider Credentials**.

```prisma
model ModelRegistryEntry {
  id          String   @id @default(cuid())

  // null = global model; non-null = org-scoped override/entry
  orgId       String?

  provider    String   // e.g. "ollama", "openai", "anthropic"
  modelName   String   // technical model ID, e.g. "llama3", "gpt-4.1-mini"

  displayName String   // human-readable name shown in UI
  description String?

  // When false, model is hidden/disabled for that org
  isEnabled   Boolean  @default(true)

  // A default suggestion for new profiles in this org
  isDefault   Boolean  @default(false)

  // Capabilities flags (chat, embedding, vision, tools, etc.)
  capabilities String[] // e.g. ["chat", "tools"]

  // Technical limits
  contextWindow Int?  // max tokens context
  maxOutputTokens Int?

  // Pricing data (micros per 1K tokens, optional)
  inputPriceMicros  Int?
  outputPriceMicros Int?

  // Free-form JSON for provider-specific metadata
  metadata    Json?

  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  org         Org?     @relation(fields: [orgId], references: [id])

  @@index([orgId])
  @@index([provider, modelName])
}

// Optional: per-org third-party provider credentials (e.g. OpenAI keys)
model OrgProviderCredential {
  id          String   @id @default(cuid())
  orgId       String

  provider    String   // "openai", "anthropic", etc.
  label       String?

  // Encrypted API key or credential blob
  apiKeyEncrypted String

  createdAt   DateTime @default(now())
  updatedAt   DateTime @updatedAt

  org         Org      @relation(fields: [orgId], references: [id])

  @@index([orgId, provider])
}
```

> **Security Note:** `apiKeyEncrypted` should be encrypted at rest (e.g. using KMS or a project‑wide encryption utility) – implementation detail lives outside this spec.

Run a Prisma migration after editing the schema.

---

## 43.3. LLM Provider Abstraction

The goal is a single, testable interface the rest of the system can call.

### 43.3.1. Types

**File:** `apps/api-gateway/src/llm/types.ts`

```ts
// apps/api-gateway/src/llm/types.ts

export type ChatRole = 'system' | 'user' | 'assistant' | 'tool';

export interface ChatMessage {
  role: ChatRole;
  content: string;
  // Future: tool call data, images, etc.
}

export interface ChatToolCall {
  id: string;
  name: string;
  arguments: string; // JSON-encoded
}

export interface ChatCompletionRequest {
  orgId: string;
  modelProvider: string; // from ChatProfile
  modelName: string;     // from ChatProfile

  messages: ChatMessage[];

  temperature?: number;
  topP?: number;
  maxTokens?: number | null;

  // Whether to stream tokens back to the client
  stream?: boolean;

  // Future: function/tool calling description, RAG metadata etc.
  tools?: any[];
  toolChoice?: 'auto' | 'none' | { name: string };

  // Contextual metadata – used for logging/metrics only
  metadata?: {
    orgId?: string;
    conversationId?: string;
    messageId?: string;
    chatProfileId?: string;
  };
}

export interface ChatCompletionChunk {
  // null content may represent a tool call or end-of-stream
  delta: string | null;
  done: boolean;

  // optional for richer providers
  toolCalls?: ChatToolCall[];
}

export interface ChatCompletionResponse {
  content: string;
  toolCalls?: ChatToolCall[];
}

export interface LlmChatProvider {
  /**
   * Non-streaming completion: returns full content.
   */
  complete(req: ChatCompletionRequest): Promise<ChatCompletionResponse>;

  /**
   * Streaming completion: yields chunks until done.
   */
  stream(req: ChatCompletionRequest): AsyncIterable<ChatCompletionChunk>;
}
```

### 43.3.2. Provider Router

**File:** `apps/api-gateway/src/llm/router.ts`

```ts
// apps/api-gateway/src/llm/router.ts

import { ChatCompletionRequest, LlmChatProvider } from './types';
import { OllamaChatProvider } from './providers/ollama';
// Future imports: OpenAIChatProvider, AnthropicChatProvider, etc.

let ollamaProvider: OllamaChatProvider | null = null;

export function initLlmProviders() {
  const ollamaBaseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
  ollamaProvider = new OllamaChatProvider({ baseUrl: ollamaBaseUrl });

  // In future: initialize other providers (OpenAI, Anthropic...) here.
}

function resolveProvider(modelProvider: string): LlmChatProvider {
  switch (modelProvider) {
    case 'ollama': {
      if (!ollamaProvider) {
        throw new Error('Ollama provider not initialized. Call initLlmProviders() first.');
      }
      return ollamaProvider;
    }
    // case 'openai': return openaiProvider;
    // case 'anthropic': return anthropicProvider;
    default:
      throw new Error(`Unsupported model provider: ${modelProvider}`);
  }
}

export async function completeWithRouting(
  req: ChatCompletionRequest
): Promise<ChatCompletionResponse> {
  const provider = resolveProvider(req.modelProvider);
  return provider.complete(req);
}

export function streamWithRouting(
  req: ChatCompletionRequest
): AsyncIterableIterator<any> {
  const provider = resolveProvider(req.modelProvider);
  return provider.stream(req) as AsyncIterableIterator<any>;
}
```

Initialize in `main.ts` (or server bootstrap):

```ts
import { initLlmProviders } from './llm/router';

initLlmProviders();
```

The **message pipeline** (25.md) will call `completeWithRouting` / `streamWithRouting` using `ChatProfile` config (42.md).

---

## 43.4. Ollama Chat Provider Implementation

### 43.4.1. Ollama Overview

We assume Ollama is reachable via HTTP:

- Base URL: `OLLAMA_BASE_URL` env (default `http://localhost:11434`).  
- Chat endpoint: `POST /api/chat`.  
- Tag listing: `GET /api/tags`.

We will use the **chat** endpoint to support conversation history, system prompt and streaming.

### 43.4.2. Provider Implementation

**File:** `apps/api-gateway/src/llm/providers/ollama.ts`

```ts
// apps/api-gateway/src/llm/providers/ollama.ts

import { ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk, LlmChatProvider } from '../types';

interface OllamaConfig {
  baseUrl: string;
}

interface OllamaChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export class OllamaChatProvider implements LlmChatProvider {
  private readonly baseUrl: string;

  constructor(config: OllamaConfig) {
    this.baseUrl = config.baseUrl.replace(/\/$/, '');
  }

  private mapMessages(messages: ChatCompletionRequest['messages']): OllamaChatMessage[] {
    return messages
      .filter((m) => m.role === 'system' || m.role === 'user' || m.role === 'assistant')
      .map((m) => ({ role: m.role as 'system' | 'user' | 'assistant', content: m.content }));
  }

  async complete(req: ChatCompletionRequest): Promise<ChatCompletionResponse> {
    const body = {
      model: req.modelName,
      messages: this.mapMessages(req.messages),
      stream: false,
      options: {
        temperature: req.temperature ?? 0.7,
        top_p: req.topP ?? 1.0,
        num_predict: req.maxTokens ?? undefined
      }
    };

    const res = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body)
    });

    if (!res.ok) {
      const text = await res.text();
      throw new Error(`Ollama chat error (${res.status}): ${text}`);
    }

    const json: any = await res.json();

    const content: string = json.message?.content ?? '';

    return { content }; // toolCalls omitted for now
  }

  async *stream(req: ChatCompletionRequest): AsyncIterable<ChatCompletionChunk> {
    const body = {
      model: req.modelName,
      messages: this.mapMessages(req.messages),
      stream: true,
      options: {
        temperature: req.temperature ?? 0.7,
        top_p: req.topP ?? 1.0,
        num_predict: req.maxTokens ?? undefined
      }
    };

    const res = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(body)
    });

    if (!res.ok || !res.body) {
      const text = await res.text().catch(() => '');
      throw new Error(`Ollama stream error (${res.status}): ${text}`);
    }

    const reader = res.body.getReader();
    const decoder = new TextDecoder('utf-8');

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        const chunkText = decoder.decode(value, { stream: true });

        // Ollama streams one JSON object per line
        const lines = chunkText.split('\n').filter(Boolean);
        for (const line of lines) {
          try {
            const data = JSON.parse(line);
            const token: string = data.message?.content ?? '';
            const doneChunk: boolean = Boolean(data.done);

            yield {
              delta: token || null,
              done: doneChunk
            };
          } catch (err) {
            // Ignore malformed lines
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    // Final completion
    yield { delta: null, done: true };
  }
}
```

> **Note:** Error handling is intentionally simple here. In production, wrap with retry/backoff logic and network timeout handling.

---

## 43.5. Using Profiles + Registry for Routing

### 43.5.1. Selecting Model Info from Registry

Before calling `completeWithRouting`, the message pipeline (25.md) should:

1. Load the **ChatProfile** (42.md) bound to the conversation.  
2. Derive `modelProvider` and `modelName` from the profile.  
3. Optionally load the **ModelRegistryEntry** for the org/provider/modelName to validate that the model is **enabled** and retrieve limits/pricing.

**Helper function:**

**File:** `apps/api-gateway/src/llm/modelRegistryService.ts`

```ts
// apps/api-gateway/src/llm/modelRegistryService.ts

import { prisma } from '@ai-chat/db';

export async function resolveModelForOrg(orgId: string, provider: string, modelName: string) {
  // Org override first
  const orgEntry = await prisma.modelRegistryEntry.findFirst({
    where: { orgId, provider, modelName }
  });

  const globalEntry = await prisma.modelRegistryEntry.findFirst({
    where: { orgId: null, provider, modelName }
  });

  const entry = orgEntry ?? globalEntry;

  if (!entry || !entry.isEnabled) {
    throw new Error(`Model ${provider}:${modelName} is not enabled for org ${orgId}`);
  }

  return entry;
}
```

### 43.5.2. Message Pipeline Integration

Within the **chat message handler** (25.md):

```ts
import { completeWithRouting } from '../llm/router';
import { resolveModelForOrg } from '../llm/modelRegistryService';

// ... inside handler after safety checks, quota checks etc.

const profile = await prisma.chatProfile.findUnique({ where: { id: chatProfileId } });
if (!profile) {
  throw new Error('Chat profile not found');
}

const modelEntry = await resolveModelForOrg(orgId, profile.modelProvider, profile.modelName);

const completion = await completeWithRouting({
  orgId,
  modelProvider: profile.modelProvider,
  modelName: profile.modelName,
  messages: builtMessages,
  temperature: profile.temperature,
  topP: profile.topP,
  maxTokens: profile.maxTokens,
  metadata: {
    orgId,
    conversationId,
    messageId: createdMessage.id,
    chatProfileId: profile.id
  }
});

// Save assistant message using completion.content
```

For streaming, call `streamWithRouting` and pipe the `ChatCompletionChunk`s into the SSE/WebSocket stream to the client.

---

## 43.6. Backend – Model Registry API Routes

We expose **org‑aware model listing and mutation** for admins.

**File:** `apps/api-gateway/src/routes/modelRegistry.ts`

```ts
// apps/api-gateway/src/routes/modelRegistry.ts

import { FastifyInstance, FastifyPluginOptions } from 'fastify';
import { z } from 'zod';
import { prisma } from '@ai-chat/db';
import { JwtPayload } from '../auth/types';
import { assertOrgPermission } from '../rbac/guards';

const upsertModelSchema = z.object({
  provider: z.string().min(1),
  modelName: z.string().min(1),
  displayName: z.string().min(1),
  description: z.string().optional(),
  isEnabled: z.boolean().optional(),
  isDefault: z.boolean().optional(),
  capabilities: z.array(z.string()).optional(),
  contextWindow: z.number().int().min(1).optional(),
  maxOutputTokens: z.number().int().min(1).optional(),
  inputPriceMicros: z.number().int().min(0).optional(),
  outputPriceMicros: z.number().int().min(0).optional(),
  metadata: z.record(z.any()).optional()
});

export default async function modelRegistryRoutes(
  app: FastifyInstance,
  _opts: FastifyPluginOptions
) {
  // List models for org (merged global + org, computed enabled flag)
  app.get('/orgs/:orgId/models', { preHandler: [app.authenticate] }, async (req, reply) => {
    const payload = req.user as JwtPayload;
    const orgId = (req.params as any).orgId as string;

    await assertOrgPermission(
      { id: payload.userId, isSuperadmin: payload.isSuperadmin },
      orgId,
      'org:models:read'
    );

    const [globalEntries, orgEntries] = await Promise.all([
      prisma.modelRegistryEntry.findMany({ where: { orgId: null } }),
      prisma.modelRegistryEntry.findMany({ where: { orgId } })
    ]);

    const merged = new Map<string, any>();

    for (const e of globalEntries) {
      const key = `${e.provider}:${e.modelName}`;
      merged.set(key, { scope: 'global', ...e });
    }

    for (const e of orgEntries) {
      const key = `${e.provider}:${e.modelName}`;
      merged.set(key, { scope: 'org', ...e });
    }

    const models = Array.from(merged.values()).map((e) => ({
      id: e.id,
      orgId: e.orgId,
      provider: e.provider,
      modelName: e.modelName,
      displayName: e.displayName,
      description: e.description,
      isEnabled: e.isEnabled,
      isDefault: e.isDefault,
      capabilities: e.capabilities,
      contextWindow: e.contextWindow,
      maxOutputTokens: e.maxOutputTokens,
      inputPriceMicros: e.inputPriceMicros,
      outputPriceMicros: e.outputPriceMicros,
      metadata: e.metadata,
      scope: e.scope
    }));

    return reply.send({ models });
  });

  // Upsert org-scoped model entry
  app.put('/orgs/:orgId/models', { preHandler: [app.authenticate] }, async (req, reply) => {
    const payload = req.user as JwtPayload;
    const orgId = (req.params as any).orgId as string;

    await assertOrgPermission(
      { id: payload.userId, isSuperadmin: payload.isSuperadmin },
      orgId,
      'org:models:write'
    );

    const parsed = upsertModelSchema.safeParse(req.body);
    if (!parsed.success) {
      return reply.code(400).send({ error: 'INVALID_BODY', details: parsed.error.format() });
    }

    const d = parsed.data;

    if (d.isDefault) {
      await prisma.modelRegistryEntry.updateMany({
        where: { orgId, provider: d.provider },
        data: { isDefault: false }
      });
    }

    const entry = await prisma.modelRegistryEntry.upsert({
      where: {
        // This requires a composite unique constraint (provider, modelName, orgId)
        // or use an explicit unique id pattern. Implementation detail can differ.
        id: `${orgId}_${d.provider}_${d.modelName}`
      },
      update: {
        displayName: d.displayName,
        description: d.description ?? null,
        isEnabled: typeof d.isEnabled === 'boolean' ? d.isEnabled : undefined,
        isDefault: typeof d.isDefault === 'boolean' ? d.isDefault : undefined,
        capabilities: d.capabilities ?? undefined,
        contextWindow: d.contextWindow ?? undefined,
        maxOutputTokens: d.maxOutputTokens ?? undefined,
        inputPriceMicros: d.inputPriceMicros ?? undefined,
        outputPriceMicros: d.outputPriceMicros ?? undefined,
        metadata: d.metadata ?? undefined
      },
      create: {
        id: `${orgId}_${d.provider}_${d.modelName}`,
        orgId,
        provider: d.provider,
        modelName: d.modelName,
        displayName: d.displayName,
        description: d.description ?? null,
        isEnabled: d.isEnabled ?? true,
        isDefault: d.isDefault ?? false,
        capabilities: d.capabilities ?? [],
        contextWindow: d.contextWindow,
        maxOutputTokens: d.maxOutputTokens,
        inputPriceMicros: d.inputPriceMicros,
        outputPriceMicros: d.outputPriceMicros,
        metadata: d.metadata ?? {}
      }
    });

    // Optional: emitEvent('model_registry.upserted', ...)

    return reply.send({ model: entry });
  });
}
```

Register in `main.ts`:

```ts
import modelRegistryRoutes from './routes/modelRegistry';

await app.register(modelRegistryRoutes);
```

> **Note:** You may instead define a composite `@@unique([orgId, provider, modelName])` in Prisma and use the standard `where` shape for `upsert`. Adjust accordingly.

---

## 43.7. Frontend – Model Registry UI (Material 3)

We add an **Org Models** page to the admin area:

- Route: `/app/orgs/:orgId/settings/models`  
- Purpose: allow org admins to see which models exist and toggle enable/default.

### 43.7.1. API Wrapper – Web

**File:** `apps/web/src/api/modelRegistry.ts`

```ts
// apps/web/src/api/modelRegistry.ts

import { apiRequest } from './client';

export interface ModelRegistryEntryDto {
  id: string;
  orgId: string | null;
  provider: string;
  modelName: string;
  displayName: string;
  description: string | null;
  isEnabled: boolean;
  isDefault: boolean;
  capabilities: string[];
  contextWindow: number | null;
  maxOutputTokens: number | null;
  inputPriceMicros: number | null;
  outputPriceMicros: number | null;
  metadata: Record<string, any> | null;
  scope: 'global' | 'org';
}

export async function fetchOrgModels(
  token: string,
  orgId: string
): Promise<{ models: ModelRegistryEntryDto[] }> {
  return apiRequest<{ models: ModelRegistryEntryDto[]}>(
    `/orgs/${orgId}/models`,
    { method: 'GET' },
    token
  );
}

export async function upsertOrgModel(
  token: string,
  orgId: string,
  model: {
    provider: string;
    modelName: string;
    displayName: string;
    description?: string;
    isEnabled?: boolean;
    isDefault?: boolean;
    capabilities?: string[];
    contextWindow?: number;
    maxOutputTokens?: number;
    inputPriceMicros?: number;
    outputPriceMicros?: number;
    metadata?: Record<string, any>;
  }
): Promise<{ model: ModelRegistryEntryDto }> {
  return apiRequest<{ model: ModelRegistryEntryDto }>(
    `/orgs/${orgId}/models`,
    {
      method: 'PUT',
      body: JSON.stringify(model)
    },
    token
  );
}
```

### 43.7.2. Org Models Settings Page

**File:** `apps/web/src/org/OrgModelsSettingsPage.tsx`

```tsx
// apps/web/src/org/OrgModelsSettingsPage.tsx

import React, { useEffect, useState } from 'react';
import {
  Box,
  Button,
  Card,
  CardContent,
  Chip,
  FormControlLabel,
  Switch,
  TextField,
  Typography
} from '@mui/material';
import AutoAwesomeIcon from '@mui/icons-material/AutoAwesome';
import RefreshIcon from '@mui/icons-material/Refresh';
import { useParams } from 'react-router-dom';
import { useAuth } from '../auth/AuthContext';
import { fetchOrgModels, ModelRegistryEntryDto, upsertOrgModel } from '../api/modelRegistry';

export const OrgModelsSettingsPage: React.FC = () => {
  const { orgId } = useParams();
  const { token } = useAuth();

  const [models, setModels] = useState<ModelRegistryEntryDto[]>([]);
  const [editing, setEditing] = useState<ModelRegistryEntryDto | null>(null);

  const gradientBg =
    'radial-gradient(circle at top left, rgba(52,211,153,0.18), transparent 55%), ' +
    'radial-gradient(circle at bottom right, rgba(59,130,246,0.18), transparent 55%)';

  const load = async () => {
    if (!token || !orgId) return;
    const res = await fetchOrgModels(token, orgId);
    setModels(res.models);
  };

  useEffect(() => {
    void load();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [orgId, token]);

  const startEdit = (entry: ModelRegistryEntryDto) => {
    setEditing(entry);
  };

  const handleSave = async () => {
    if (!token || !orgId || !editing) return;

    await upsertOrgModel(token, orgId, {
      provider: editing.provider,
      modelName: editing.modelName,
      displayName: editing.displayName,
      description: editing.description ?? undefined,
      isEnabled: editing.isEnabled,
      isDefault: editing.isDefault,
      capabilities: editing.capabilities,
      contextWindow: editing.contextWindow ?? undefined,
      maxOutputTokens: editing.maxOutputTokens ?? undefined,
      inputPriceMicros: editing.inputPriceMicros ?? undefined,
      outputPriceMicros: editing.outputPriceMicros ?? undefined,
      metadata: editing.metadata ?? undefined
    });

    setEditing(null);
    await load();
  };

  return (
    <Box
      sx={{
        p: 2,
        display: 'flex',
        flexDirection: 'column',
        gap: 2,
        height: '100%',
        backgroundImage: gradientBg,
        backgroundColor: 'background.default'
      }}
    >
      <Box display="flex" alignItems="center" justifyContent="space-between">
        <Box display="flex" alignItems="center" gap={1}>
          <AutoAwesomeIcon fontSize="small" />
          <Box>
            <Typography variant="h6">Models & providers</Typography>
            <Typography variant="caption" color="text.secondary">
              Control which models are available for chat profiles in this organization.
            </Typography>
          </Box>
        </Box>
        <Button
          size="small"
          startIcon={<RefreshIcon />}
          onClick={() => void load()}
        >
          Refresh
        </Button>
      </Box>

      <Card sx={{ borderRadius: 3, flex: 1, overflow: 'auto' }}>
        <CardContent sx={{ display: 'flex', flexDirection: 'column', gap: 1.5 }}>
          {models.length === 0 && (
            <Typography variant="body2" color="text.secondary">
              No models configured yet.
            </Typography>
          )}

          {models.map((m) => (
            <Box
              key={m.id}
              sx={{
                p: 1.25,
                borderRadius: 2,
                border: '1px solid',
                borderColor: m.isEnabled ? 'divider' : 'action.disabledBackground',
                display: 'flex',
                flexDirection: 'column',
                gap: 0.5
              }}
            >
              <Box display="flex" justifyContent="space-between" alignItems="center">
                <Box display="flex" flexDirection="column">
                  <Typography variant="body1">{m.displayName}</Typography>
                  <Typography variant="caption" color="text.secondary">
                    {m.provider}:{m.modelName} · {m.scope === 'org' ? 'Org override' : 'Global'}
                  </Typography>
                </Box>
                <Box display="flex" alignItems="center" gap={1}>
                  {m.isDefault && <Chip size="small" label="Default" />}
                  {!m.isEnabled && <Chip size="small" label="Disabled" />}
                  <Button size="small" onClick={() => startEdit(m)}>
                    Edit
                  </Button>
                </Box>
              </Box>

              {m.description && (
                <Typography variant="body2" color="text.secondary">
                  {m.description}
                </Typography>
              )}

              <Box display="flex" flexWrap="wrap" gap={0.5} mt={0.5}>
                {m.capabilities.map((c) => (
                  <Chip key={c} size="small" label={c} />
                ))}
                {m.contextWindow && (
                  <Chip size="small" label={`Context: ${m.contextWindow}`} />
                )}
                {m.maxOutputTokens && (
                  <Chip size="small" label={`Max out: ${m.maxOutputTokens}`} />
                )}
                {m.inputPriceMicros != null && (
                  <Chip
                    size="small"
                    label={`In: $${(m.inputPriceMicros / 1_000_000).toFixed(4)}/1K`}
                  />
                )}
                {m.outputPriceMicros != null && (
                  <Chip
                    size="small"
                    label={`Out: $${(m.outputPriceMicros / 1_000_000).toFixed(4)}/1K`}
                  />
                )}
              </Box>
            </Box>
          ))}
        </CardContent>
      </Card>

      {editing && (
        <Card
          sx={{
            borderRadius: 3,
            p: 2,
            position: 'fixed',
            bottom: 16,
            right: 16,
            width: 420,
            boxShadow: 8
          }}
        >
          <Typography variant="subtitle1" gutterBottom>
            Edit model – {editing.displayName}
          </Typography>
          <TextField
            label="Display name"
            fullWidth
            margin="dense"
            value={editing.displayName}
            onChange={(e) =>
              setEditing((prev) => (prev ? { ...prev, displayName: e.target.value } : prev))
            }
          />
          <TextField
            label="Description"
            fullWidth
            margin="dense"
            value={editing.description || ''}
            onChange={(e) =>
              setEditing((prev) => (prev ? { ...prev, description: e.target.value } : prev))
            }
          />
          <FormControlLabel
            control={
              <Switch
                checked={editing.isEnabled}
                onChange={(e) =>
                  setEditing((prev) =>
                    prev ? { ...prev, isEnabled: e.target.checked } : prev
                  )
                }
              />
            }
            label="Enabled for this org"
          />
          <FormControlLabel
            control={
              <Switch
                checked={editing.isDefault}
                onChange={(e) =>
                  setEditing((prev) =>
                    prev ? { ...prev, isDefault: e.target.checked } : prev
                  )
                }
              />
            }
            label="Use as default model for this provider"
          />

          <Box display="flex" justifyContent="flex-end" gap={1} mt={1.5}>
            <Button size="small" onClick={() => setEditing(null)}>
              Cancel
            </Button>
            <Button size="small" variant="contained" onClick={() => void handleSave()}>
              Save
            </Button>
          </Box>
        </Card>
      )}
    </Box>
  );
};
```

Add to router:

```tsx
import { OrgModelsSettingsPage } from './org/OrgModelsSettingsPage';

<Route path="/app/orgs/:orgId/settings/models" element={<OrgModelsSettingsPage />} />
```

---

## 43.8. Observability & Metrics

Integrate the model layer with metrics (28.md):

- `llm_requests_total{provider,modelName,orgId}` – count of LLM requests.  
- `llm_tokens_input_total{provider,modelName,orgId}` – estimated input tokens (if available).  
- `llm_tokens_output_total{provider,modelName,orgId}` – estimated output tokens.  
- `llm_request_duration_ms{provider,modelName}` – histogram of request latencies.  
- `ollama_health_status{orgId}` – optional, based on a periodic health check to `/api/tags`.

Emit events (38.md):

- `model_registry.upserted` – whenever an org modifies a model entry.  
- `llm.request` & `llm.response` (with sensitive data stripped/truncated) for debugging and audit.

Grafana dashboard ideas:

- Per‑org breakdown: which models are used most.  
- Performance & error rate per provider/model.  
- Cost estimation per org using price metadata.

---

## 43.9. Ollama Deployment Notes (Non‑binding)

Although deployment is out of scope, keep in mind:

- Ollama may run **on the same host** as the API or on a separate GPU machine.  
- Use `OLLAMA_BASE_URL` to point the API gateway to the right host (internal network, not public internet).  
- Consider adding a periodic **health check job** that hits `/api/tags` and updates a `ollama_health_status` metric.

This spec does not enforce deployment strategy; it just ensures the code is ready for production deployments.

---

## 43.10. Sanity Checklist

Before enabling multi‑provider & Ollama in production:

- [ ] Prisma migration for `ModelRegistryEntry` and `OrgProviderCredential` applied.  
- [ ] `initLlmProviders()` is called on server startup and `OLLAMA_BASE_URL` is correctly configured.  
- [ ] `completeWithRouting` / `streamWithRouting` used instead of direct provider calls.  
- [ ] Chat pipeline resolves models via `ChatProfile` + `ModelRegistryEntry`.  
- [ ] Org Models admin UI (`/settings/models`) loads and updates model entries.  
- [ ] Metrics & events for LLM calls are visible in Grafana & audit log.

If all checks pass, the AI chat platform now has a **modern model layer**: multi‑provider support, Org‑aware model governance and first‑class **Ollama** integration, on par with industry‑leading AI platforms.

---

_End of 43.md – Model Registry, Provider Routing & Ollama Integration – Backend + Material 3 UI_
