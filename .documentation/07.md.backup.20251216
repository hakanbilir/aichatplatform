

# 7. Chat Orchestrator – Prompt Building & Streaming Flow

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human developers.  
> **Goal of this file:** Implement the **chat orchestration layer** in `@ai-chat/chat-orchestrator` so that backend apps can:
>
> - Build prompts from system messages, custom instructions, and history.  
> - Enforce simple context window limits (truncate history).  
> - Call Ollama via `@ai-chat/ollama-client` for both streaming and non-streaming chat.  
> - Emit a clean `AsyncGenerator<ChatStreamEvent>` that API routes can expose over SSE/WebSockets.
>
> This file focuses on **LLM orchestration**, not persistence. Database read/write logic will be handled by the API layer in later docs.

> **Important instructions to AI agents:**
> - Keep the orchestrator **stateless**: it should not talk to the DB directly.  
> - Use only the types and helpers from `@ai-chat/core-types` and `@ai-chat/ollama-client`.  
> - Do **not** implement tools/function-calling logic yet; but design the orchestrator API so tools can be added later without breaking changes.

---

## 7.1. Role of the Chat Orchestrator

The **chat orchestrator** is the brain between:

- Raw inputs from the API (user message, conversation history, settings).  
- The LLM runtime (Ollama).  
- Streaming output (tokens) sent back to the frontend.

Responsibilities:

1. **Prompt construction**
   - Combine:
     - System prompt (if any).
     - User-level custom instructions (if any).
     - Conversation history.
     - Current user input.

2. **Context management**
   - Enforce a maximum approximate context size.  
   - Trim oldest messages first when necessary.

3. **Model invocation**
   - Call `createChatCompletion` (non-streaming) or `streamChatCompletion` (streaming) from `@ai-chat/ollama-client`.

4. **Streaming abstraction**
   - Expose an `AsyncGenerator<ChatStreamEvent>` that API routes can transform into SSE/WebSocket messages.

5. **Extensibility**
   - Accept optional tools and RAG snippets now, even if they are not fully processed.  
   - Later docs will expand the orchestrator to:
     - Decide when to call tools.
     - Inject tool results into the message stream.
     - Integrate knowledge base context.

---

## 7.2. Orchestrator Package Overview

The orchestrator lives in `packages/chat-orchestrator` and will export:

- Types:
  - `ConversationContext` – what the orchestrator knows about the ongoing conversation.
  - `OrchestratorOptions` – global options like max context length.
  - `ChatRunParams` – per-request parameters (model, temperature, etc.).
- Functions:
  - `buildPromptMessages(...)` – construct final message array for the LLM.
  - `runChatCompletion(...)` – non-streaming call that returns a `ChatCompletionResponse`.
  - `streamChatCompletionOrchestrated(...)` – streaming call returning `AsyncGenerator<ChatStreamEvent>`.

Later docs may add more advanced orchestration functions (e.g. multi-step tool loops, RAG integration), but we design these APIs to be forwards-compatible.

---

## 7.3. Update `packages/chat-orchestrator/package.json`

We need dependencies on `@ai-chat/core-types`, `@ai-chat/ollama-client`, and `@ai-chat/config`.

Replace `packages/chat-orchestrator/package.json` with:

```json
{
  "name": "@ai-chat/chat-orchestrator",
  "version": "0.1.0",
  "private": true,
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "lint": "eslint src --ext .ts",
    "test": "echo \"no tests yet\"",
    "clean": "rm -rf dist"
  },
  "dependencies": {
    "@ai-chat/config": "0.1.0",
    "@ai-chat/core-types": "0.1.0",
    "@ai-chat/ollama-client": "0.1.0"
  }
}
```

Ensure `packages/chat-orchestrator/tsconfig.json` is already using the shared template (from 3.md). If not, update it to:

```json
{
  "extends": "../../tsconfig.base.json",
  "compilerOptions": {
    "outDir": "dist",
    "declaration": true,
    "declarationMap": true
  },
  "include": ["src"]
}
```

---

## 7.4. Orchestrator Types & Interfaces

We now implement the orchestrator logic in `packages/chat-orchestrator/src/index.ts`.

> **AI Agent Note:**  
> Replace the entire contents of `packages/chat-orchestrator/src/index.ts` with the code in this section.

Create/replace `packages/chat-orchestrator/src/index.ts` with:

```ts
import { getConfig } from '@ai-chat/config';
import {
  ChatCompletionParams,
  ChatCompletionResponse,
  ChatMessage,
  ChatStreamEvent,
  ChatStreamEventType,
  ChatRole,
  ToolDefinition
} from '@ai-chat/core-types';
import {
  createChatCompletion as createOllamaChatCompletion,
  streamChatCompletion as streamOllamaChatCompletion
} from '@ai-chat/ollama-client';

const config = getConfig();

// =========================
// Orchestrator Types
// =========================

/**
 * High-level context of a conversation known to the orchestrator.
 *
 * This is intentionally DB-agnostic: it does not include ORM types.
 */
export interface ConversationContext {
  /**
   * A stable conversation identifier (may be a DB ID or transient ID).
   */
  id: string;
  /**
   * Optional title for logging / future use.
   */
  title?: string;
  /**
   * System-level instructions for this conversation.
   */
  systemPrompt?: string;
  /**
   * User-specific custom instructions.
   */
  customInstructions?: string;
  /**
   * Historical messages for the conversation.
   */
  history: ChatMessage[];
}

/**
 * Options controlling how the orchestrator behaves.
 */
export interface OrchestratorOptions {
  /**
   * Approximate maximum context size in tokens.
   * We will use a simple heuristic based on character length for now.
   */
  maxContextTokens: number;
}

/**
 * Parameters for a single chat run (one user message).
 */
export interface ChatRunParams {
  model?: string;
  temperature?: number;
  topP?: number;
  maxTokens?: number;
  /**
   * Optional tools available to the model.
   * Tools are not executed yet; this is for future extension.
   */
  tools?: ToolDefinition[];
  /**
   * Optional AbortSignal to cancel streaming.
   */
  signal?: AbortSignal;
}

/**
 * Internal options with defaults applied.
 */
interface ResolvedChatRunParams extends ChatRunParams {
  model: string;
}

// =========================
// Helper Functions
// =========================

/**
 * Very rough heuristic to estimate token count from message content length.
 *
 * We assume ~4 characters per token as a simple rule of thumb.
 */
function estimateTokens(text: string): number {
  const length = text.length;
  if (length === 0) return 0;
  return Math.max(1, Math.round(length / 4));
}

/**
 * Estimate approximate total tokens for a list of messages.
 */
function estimateTotalTokens(messages: ChatMessage[]): number {
  return messages.reduce((sum, msg) => sum + estimateTokens(msg.content), 0);
}

/**
 * Trim messages so that the approximate token count does not exceed maxTokens.
 *
 * Oldest messages are removed first (from the beginning), keeping the latest messages + user input.
 */
function trimMessagesToMaxTokens(messages: ChatMessage[], maxTokens: number): ChatMessage[] {
  let result = [...messages];
  let total = estimateTotalTokens(result);

  // Keep at least the last message even if it exceeds max on its own.
  while (result.length > 1 && total > maxTokens) {
    result.shift();
    total = estimateTotalTokens(result);
  }

  return result;
}

/**
 * Build the final message list to send to the LLM from:
 * - Conversation system prompt
 * - User custom instructions
 * - Conversation history
 * - Current user message
 */
export function buildPromptMessages(
  context: ConversationContext,
  userMessage: ChatMessage
): ChatMessage[] {
  const messages: ChatMessage[] = [];

  if (context.systemPrompt) {
    messages.push({
      role: 'system',
      content: context.systemPrompt
    });
  }

  if (context.customInstructions) {
    messages.push({
      role: 'system',
      content: context.customInstructions,
      name: 'custom_instructions'
    });
  }

  // Append existing history
  for (const msg of context.history) {
    messages.push(msg);
  }

  // Append the new user message
  messages.push(userMessage);

  return messages;
}

/**
 * Apply default values to ChatRunParams using config defaults.
 */
function resolveChatRunParams(params: ChatRunParams): ResolvedChatRunParams {
  return {
    model: params.model ?? config.DEFAULT_MODEL,
    temperature: params.temperature ?? 0.7,
    topP: params.topP ?? 1,
    maxTokens: params.maxTokens,
    tools: params.tools,
    signal: params.signal
  };
}

// =========================
// Orchestrated Non-Streaming Chat
// =========================

/**
 * Run a non-streaming chat completion through Ollama.
 *
 * Use this for background jobs or where streaming is not required.
 */
export async function runChatCompletion(
  context: ConversationContext,
  userMessage: ChatMessage,
  options: OrchestratorOptions,
  params: ChatRunParams = {}
): Promise<ChatCompletionResponse> {
  const resolvedParams = resolveChatRunParams(params);

  const allMessages = buildPromptMessages(context, userMessage);
  const trimmed = trimMessagesToMaxTokens(allMessages, options.maxContextTokens);

  const completionParams: ChatCompletionParams = {
    model: resolvedParams.model,
    messages: trimmed,
    temperature: resolvedParams.temperature,
    topP: resolvedParams.topP,
    maxTokens: resolvedParams.maxTokens,
    tools: resolvedParams.tools
  };

  const result = await createOllamaChatCompletion(completionParams);
  return result;
}

// =========================
// Orchestrated Streaming Chat
// =========================

/**
 * Orchestrated streaming chat.
 *
 * This wraps `streamOllamaChatCompletion` and is responsible for:
 * - Building and trimming the prompt messages.
 * - Forwarding streaming events.
 * - Ensuring at least one `start` and one `end` or `error` event.
 */
export async function* streamChatCompletionOrchestrated(
  context: ConversationContext,
  userMessage: ChatMessage,
  options: OrchestratorOptions,
  params: ChatRunParams = {}
): AsyncGenerator<ChatStreamEvent, void, unknown> {
  const resolvedParams = resolveChatRunParams(params);

  const allMessages = buildPromptMessages(context, userMessage);
  const trimmed = trimMessagesToMaxTokens(allMessages, options.maxContextTokens);

  const completionParams: ChatCompletionParams & { signal?: AbortSignal } = {
    model: resolvedParams.model,
    messages: trimmed,
    temperature: resolvedParams.temperature,
    topP: resolvedParams.topP,
    maxTokens: resolvedParams.maxTokens,
    tools: resolvedParams.tools,
    signal: resolvedParams.signal
  };

  let started = false;
  let endedOrErrored = false;

  try {
    for await (const event of streamOllamaChatCompletion(completionParams)) {
      if (!started) {
        // Ensure a single 'start' event is emitted once at the beginning.
        started = true;
        yield { type: 'start' };
      }

      // Forward events from the underlying client, but normalize types if needed.
      if (event.type === 'start') {
        // Ignore underlying start; we've already emitted our own.
        continue;
      }

      if (event.type === 'token' || event.type === 'end' || event.type === 'error') {
        if (event.type === 'end' || event.type === 'error') {
          endedOrErrored = true;
        }
        yield event;
      }
    }

    if (!started) {
      // If underlying stream produced no events, still emit a start.
      started = true;
      yield { type: 'start' };
    }

    if (!endedOrErrored) {
      // Ensure an 'end' event if none was emitted.
      const fallbackMessage: ChatMessage = {
        role: 'assistant',
        content: ''
      };
      yield {
        type: 'end',
        finalMessage: fallbackMessage
      };
    }
  } catch (err) {
    if (!started) {
      yield { type: 'start' };
      started = true;
    }

    const errorMessage = (err as Error).message ?? 'Unknown error during streaming chat';
    yield {
      type: 'error',
      error: errorMessage
    };
  }
}

// =========================
// Convenience: Create a user message
// =========================

/**
 * Helper to create a user message from plain text.
 */
export function createUserMessage(content: string): ChatMessage {
  const msg: ChatMessage = {
    role: 'user' as ChatRole,
    content
  };
  return msg;
}
```

> **Result:**  
> The orchestrator now provides high-level functions for building prompts, enforcing context limits, and delegating to Ollama for both streaming and non-streaming chat.

---

## 7.5. Example Usage (API Layer Preview)

> **Note:** This section illustrates how the API gateway might use the orchestrator. The actual API route implementation will be defined in a later markdown file.

Example (conceptual) usage inside an API handler:

```ts
import { prisma } from '@ai-chat/db';
import {
  ConversationContext,
  OrchestratorOptions,
  streamChatCompletionOrchestrated,
  createUserMessage
} from '@ai-chat/chat-orchestrator';

const orchestratorOptions: OrchestratorOptions = {
  maxContextTokens: 4000
};

async function exampleHandler() {
  // Fetch conversation + messages from DB (pseudocode)
  const conversation = await prisma.conversation.findUnique({
    where: { id: 'some-id' },
    include: { messages: true }
  });

  if (!conversation) {
    throw new Error('Conversation not found');
  }

  const context: ConversationContext = {
    id: conversation.id,
    title: conversation.title ?? undefined,
    systemPrompt: conversation.systemPrompt ?? undefined,
    customInstructions: undefined, // can be populated from user profile later
    history: conversation.messages.map((m) => ({
      id: m.id,
      role: m.role.toLowerCase() as any,
      content: m.content,
      createdAt: m.createdAt.toISOString()
    }))
  };

  const userMessage = createUserMessage('Hello, world!');

  for await (const event of streamChatCompletionOrchestrated(
    context,
    userMessage,
    orchestratorOptions,
    { model: conversation.model }
  )) {
    // Convert ChatStreamEvent to SSE/WebSocket messages...
    console.log(event);
  }
}
```

> **AI Agent Note:**  
> Do **not** implement this handler yet. It is **illustrative only** and will be formalized in the API markdown.

---

## 7.6. Sanity Checks

After implementing the orchestrator, perform the following from the repo root:

1. **Install dependencies (if not already done):**

   ```bash
   pnpm install
   ```

2. **Build the monorepo:**

   ```bash
   pnpm build
   ```

   - This should compile `@ai-chat/chat-orchestrator` without TypeScript errors.

3. **Optional quick test (temporary script):**

   Create `apps/api-gateway/src/test-orchestrator.ts`:

   ```ts
   /* eslint-disable no-console */

   import { ConversationContext, createUserMessage, streamChatCompletionOrchestrated } from '@ai-chat/chat-orchestrator';
   import { OrchestratorOptions } from '@ai-chat/chat-orchestrator';

   const context: ConversationContext = {
     id: 'test-conv',
     title: 'Test Conversation',
     systemPrompt: 'You are a helpful assistant.',
     customInstructions: 'Always respond concisely.',
     history: []
   };

   const options: OrchestratorOptions = {
     maxContextTokens: 4000
   };

   async function main() {
     const userMessage = createUserMessage('Say hello in one short sentence.');

     for await (const event of streamChatCompletionOrchestrated(context, userMessage, options, {})) {
       console.log(event);
     }
   }

   main().catch((err) => {
     console.error(err);
     process.exit(1);
   });
   ```

   Then run (if you add a script or call ts-node directly):

   ```bash
   pnpm --filter api-gateway ts-node src/test-orchestrator.ts
   ```

   Expected:
   - You see a `start` event, multiple `token` events, and finally an `end` event with the accumulated message.

---

## 7.7. Next Steps

With the chat orchestrator implemented:

- The backend has a **clean LLM interface** with:
  - Prompt building and context trimming.  
  - Streaming and non-streaming completions.
- The orchestrator is decoupled from persistence and tools, ready to be extended.

Upcoming markdown files will:

- Implement **auth & user/org APIs** using `@ai-chat/db`.  
- Implement **chat & conversation APIs** that:
  - Persist messages and conversations.  
  - Use `streamChatCompletionOrchestrated` to stream responses via SSE.  
- Implement the **tools engine integration** into the orchestrator.

> **AI Agent Instruction:**  
> Before moving on, ensure `pnpm build` runs cleanly and that the orchestrator functions compile successfully.

---

_End of 7.md – Chat Orchestrator – Prompt Building & Streaming Flow_
