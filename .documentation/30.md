**Note:** This project has been migrated from Docker to PM2 process management. See `/root/PM2_MIGRATION_GUIDE.md` for details.

# 30. Production Deployment, Configuration & Scaling Guide

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human operators.  
> **Goal of this file:** Provide a **complete, production‑grade deployment guide** for the AI chat platform with Ollama integration.
>
> - Standardize configuration and environment variables across services.  
> - Provide **Docker / Docker Compose** setup for local & single‑host production.  
> - Define patterns that can be lifted into **Kubernetes** or other orchestrators.  
> - Document health checks, readiness, and rolling deployments.  
> - Ensure **Cursor‑like agents** can provision a working deployment from scratch.

This file assumes:

- Backend: `apps/api-gateway` (Fastify, Node.js, Prisma, Ollama provider).  
- Frontend: `apps/web` (React + Material 3).  
- Database: PostgreSQL.  
- AI backend: Ollama (local or remote).

---

## 30.1. Environment & Configuration Model

We follow a **12‑factor style** configuration using environment variables. All services should read config exclusively from env.

### 30.1.1. Top‑Level Environment Variables

Define the following env variables at the **deployment root** (for example, in `.env` for Docker Compose):

```env
# === Shared base configuration ===
NODE_ENV=production
LOG_LEVEL=info

# === PostgreSQL ===
DATABASE_URL=postgresql://ai_chat_user:ai_chat_password@db:5432/ai_chat_db?schema=public

# === JWT / Auth ===
JWT_SECRET=change_this_in_prod

# === API Gateway ===
API_HTTP_PORT=4000
API_PUBLIC_URL=http://localhost:4000

# If behind a reverse proxy, this should be the external URL
APP_PUBLIC_URL=http://localhost:3000

# === Ollama ===
OLLAMA_BASE_URL=http://ollama:11434

# === CORS ===
CORS_ORIGINS=http://localhost:3000

# === Rate limits & safety (can be tuned per environment) ===
ORG_HARD_TOKEN_LIMIT=500000
ORG_SOFT_TOKEN_LIMIT=250000

# === Observability ===
PROMETHEUS_ENABLED=true
```

> **AI agent note:** When generating deployment configs, always ensure `DATABASE_URL`, `JWT_SECRET`, and `OLLAMA_BASE_URL` are set correctly for the target environment.

### 30.1.2. API Gateway Config Loader

The API gateway should have a small module centralizing configuration.

**File:** `apps/api-gateway/src/config/env.ts`

```ts
// apps/api-gateway/src/config/env.ts

export interface ApiConfig {
  nodeEnv: 'development' | 'test' | 'production';
  port: number;
  publicUrl: string;
  appPublicUrl: string;
  databaseUrl: string;
  jwtSecret: string;
  corsOrigins: string[];
  orgHardTokenLimit: number;
  orgSoftTokenLimit: number;
  prometheusEnabled: boolean;
}

function parseBoolean(val: string | undefined, defaultValue: boolean): boolean {
  if (typeof val === 'undefined') return defaultValue;
  const normalized = val.toLowerCase();
  return normalized === 'true' || normalized === '1' || normalized === 'yes';
}

function parseNumber(val: string | undefined, defaultValue: number): number {
  if (!val) return defaultValue;
  const n = Number(val);
  return Number.isFinite(n) ? n : defaultValue;
}

export function loadApiConfig(): ApiConfig {
  const nodeEnv = (process.env.NODE_ENV as ApiConfig['nodeEnv']) || 'development';

  const port = parseNumber(process.env.API_HTTP_PORT, 8080);
  const publicUrl = process.env.API_PUBLIC_URL || `http://localhost:${port}`;
  const appPublicUrl = process.env.APP_PUBLIC_URL || 'http://localhost:3000';
  const databaseUrl = process.env.DATABASE_URL || '';
  const jwtSecret = process.env.JWT_SECRET || '';
  const corsOriginsRaw = process.env.CORS_ORIGINS || appPublicUrl;
  const corsOrigins = corsOriginsRaw
    .split(',')
    .map((x) => x.trim())
    .filter(Boolean);

  const orgHardTokenLimit = parseNumber(process.env.ORG_HARD_TOKEN_LIMIT, 500000);
  const orgSoftTokenLimit = parseNumber(process.env.ORG_SOFT_TOKEN_LIMIT, 250000);

  const prometheusEnabled = parseBoolean(process.env.PROMETHEUS_ENABLED, true);

  if (!databaseUrl) {
    throw new Error('DATABASE_URL is required');
  }

  if (!jwtSecret) {
    throw new Error('JWT_SECRET is required');
  }

  return {
    nodeEnv,
    port,
    publicUrl,
    appPublicUrl,
    databaseUrl,
    jwtSecret,
    corsOrigins,
    orgHardTokenLimit,
    orgSoftTokenLimit,
    prometheusEnabled
  };
}
```

In `main.ts`, call `loadApiConfig()` and wire it into Fastify.

---

## 30.2. Docker Images

We provide **multi‑stage Dockerfiles** for API gateway and web app.

### 30.2.1. API Gateway Dockerfile

**File:** `apps/api-gateway/Dockerfile`

```dockerfile
# apps/api-gateway/Dockerfile

FROM node:22-alpine AS base
WORKDIR /app

# Install core build deps (pnpm etc.) if you use a monorepo
RUN corepack enable

# === Build stage ===
FROM base AS build

# Copy monorepo files (adjust paths according to your repo structure)
COPY package.json pnpm-lock.yaml pnpm-workspace.yaml ./
COPY apps ./apps
COPY packages ./packages

RUN pnpm install --frozen-lockfile

# Build the api-gateway package
RUN pnpm --filter api-gateway run build

# === Runtime stage ===
FROM node:22-alpine AS runtime
WORKDIR /app

ENV NODE_ENV=production

# Copy node_modules and built files from build stage
COPY --from=build /app/node_modules ./node_modules
COPY --from=build /app/apps/api-gateway/dist ./dist

# Expose HTTP port
EXPOSE 8080

CMD ["node", "dist/main.js"]
```

> **Note:** If your build output path differs (e.g. `build` instead of `dist`), adjust accordingly.

### 30.2.2. Web App Dockerfile

**File:** `apps/web/Dockerfile`

```dockerfile
# apps/web/Dockerfile

FROM node:22-alpine AS base
WORKDIR /app
RUN corepack enable

FROM base AS build

COPY package.json pnpm-lock.yaml pnpm-workspace.yaml ./
COPY apps ./apps
COPY packages ./packages

RUN pnpm install --frozen-lockfile

# Build the web app for production
RUN pnpm --filter web run build

# === Static file server (nginx) ===
FROM nginx:1.27-alpine AS runtime

# Copy build output to nginx html dir (adjust path if your build directory differs)
COPY --from=build /app/apps/web/dist /usr/share/nginx/html

# Basic nginx config can be customized if necessary
EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

If your React app builds to `build` instead of `dist`, update the path accordingly.

---

## 30.3. Docker Compose – All‑In‑One Stack

We provide a `docker-compose.yml` file that runs:

- PostgreSQL  
- Ollama  
- API gateway  
- Web UI

**File:** `docker-compose.yml` at repo root

```yaml
version: '3.9'

services:
  db:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ai_chat_user
      POSTGRES_PASSWORD: ai_chat_password
      POSTGRES_DB: ai_chat_db
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - '5432:5432'

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - '11434:11434'

  api-gateway:
    build:
      context: .
      dockerfile: apps/api-gateway/Dockerfile
    depends_on:
      - db
      - ollama
    environment:
      NODE_ENV: production
      DATABASE_URL: postgresql://ai_chat_user:ai_chat_password@db:5432/ai_chat_db?schema=public
      JWT_SECRET: change_this_in_prod
      API_HTTP_PORT: 8080
      API_PUBLIC_URL: http://api-gateway:8080
      APP_PUBLIC_URL: http://localhost:3000
      OLLAMA_BASE_URL: http://ollama:11434
      CORS_ORIGINS: http://localhost:3000
      ORG_HARD_TOKEN_LIMIT: '500000'
      ORG_SOFT_TOKEN_LIMIT: '250000'
      PROMETHEUS_ENABLED: 'true'
    ports:
      - '8080:8080'

  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    depends_on:
      - api-gateway
    environment:
      # If your frontend needs API base URL at build time, bake it via build args or env
      API_BASE_URL: http://localhost:4000
    ports:
      - '3000:80'

volumes:
  db_data:
  ollama_data:
```

### 30.3.1. Compose Workflow

From repo root:

```bash
# 1) Build and start all services
docker compose up --build

# 2) Apply database migrations (inside api-gateway container if needed)
docker compose exec api-gateway pnpm prisma migrate deploy

# 3) Access services
# Web UI:        http://localhost:3000
# API Gateway:   http://localhost:4000
# Prometheus metrics (API): http://localhost:4000/metrics
```

> **AI agent note:** When scripting this setup, ensure the migration step runs once onboarding is complete.

---

## 30.4. Database Migration & Seeding

Prisma migrations must run on each deployment to keep schema in sync.

### 30.4.1. Migration Commands

From repo root (locally or inside the `api-gateway` container):

```bash
# Development migration (interactive)
pnpm --filter api-gateway prisma migrate dev

# Production migration
pnpm --filter api-gateway prisma migrate deploy
```

### 30.4.2. Seed Script

If you maintain a seed script (example below), ensure it’s idempotent.

**File:** `apps/api-gateway/prisma/seed.ts`

```ts
// apps/api-gateway/prisma/seed.ts

import { prisma } from '@ai-chat/db';

async function main() {
  // Example: create a default org and admin user if none exist
  const existingOrgs = await prisma.org.count();
  if (existingOrgs === 0) {
    const org = await prisma.org.create({
      data: {
        name: 'Default Org'
      }
    });

    await prisma.user.create({
      data: {
        email: 'admin@example.com',
        displayName: 'Admin',
        passwordHash: 'CHANGE_ME', // In real deployments you should set this manually or via invite flow
        orgMemberships: {
          create: {
            orgId: org.id,
            role: 'OWNER'
          }
        }
      }
    });
  }
}

main()
  .then(async () => {
    await prisma.$disconnect();
  })
  .catch(async (e) => {
    console.error(e);
    await prisma.$disconnect();
    process.exit(1);
  });
```

Run with:

```bash
pnpm --filter api-gateway prisma db seed
```

> **Security note:** Never ship real passwords in code. Use this only as an example and override credentials in real environments.

---

## 30.5. Health Checks, Readiness & Liveness

To support load balancers and orchestrators, we expose:

- `GET /healthz` – basic liveness (process up, event loop responsive).  
- `GET /readyz` – readiness (DB reachable, migrations applied, Ollama accessible).

### 30.5.1. Health Routes

**File:** `apps/api-gateway/src/routes/health.ts`

```ts
// apps/api-gateway/src/routes/health.ts

import { FastifyInstance, FastifyPluginOptions } from 'fastify';
import { prisma } from '@ai-chat/db';

export default async function healthRoutes(app: FastifyInstance, _opts: FastifyPluginOptions) {
  app.get('/healthz', async (_request, reply) => {
    return reply.send({ status: 'ok' });
  });

  app.get('/readyz', async (_request, reply) => {
    try {
      await prisma.$queryRaw`SELECT 1`;
    } catch (err) {
      return reply.code(503).send({ status: 'error', reason: 'db_unreachable' });
    }

    // Optionally: check Ollama
    try {
      const res = await fetch((process.env.OLLAMA_BASE_URL || 'http://localhost:11434') + '/api/tags');
      if (!res.ok) {
        return reply.code(503).send({ status: 'error', reason: 'ollama_unhealthy' });
      }
    } catch {
      return reply.code(503).send({ status: 'error', reason: 'ollama_unreachable' });
    }

    return reply.send({ status: 'ok' });
  });
}
```

In `main.ts`:

```ts
import healthRoutes from './routes/health';

await app.register(healthRoutes);
```

These endpoints can be used by Kubernetes, Docker healthcheck, or external probes.

---

## 30.6. Kubernetes Deployment Blueprint (Conceptual)

While exact manifests depend on your cluster, we outline a minimal blueprint that AI agents can expand.

### 30.6.1. Secrets & ConfigMaps

- **Secrets:** database url, JWT secret, any external API keys.  
- **ConfigMaps:** non‑secret config like `API_HTTP_PORT`, `APP_PUBLIC_URL`, `CORS_ORIGINS`, and feature flags.

Example placeholders (not full YAML):

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ai-chat-secrets
stringData:
  DATABASE_URL: postgresql://ai_chat_user:ai_chat_password@db:5432/ai_chat_db?schema=public
  JWT_SECRET: change_this_in_prod
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-chat-config
data:
  API_HTTP_PORT: "8080"
  API_PUBLIC_URL: "https://api.example.com"
  APP_PUBLIC_URL: "https://app.example.com"
  OLLAMA_BASE_URL: "http://ollama:11434"
  CORS_ORIGINS: "https://app.example.com"
```

### 30.6.2. API Deployment & Service

Key points for a `Deployment`:

- Container image: `your-registry/api-gateway:TAG`.  
- Env from Secret + ConfigMap.  
- `readinessProbe` hitting `/readyz`.  
- `livenessProbe` hitting `/healthz`.  
- Resource requests/limits based on expected traffic.

Example snippet (simplified):

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
        - name: api-gateway
          image: your-registry/api-gateway:latest
          ports:
            - containerPort: 8080
          envFrom:
            - secretRef:
                name: ai-chat-secrets
            - configMapRef:
                name: ai-chat-config
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 20
```

A `Service` would expose it internally; an Ingress or gateway would expose it externally.

### 30.6.3. Web App Deployment

Deploy the web app as a standard static web container (e.g., nginx) fronted by an Ingress. API base URL should be configured via environment at build time or via a small runtime config JS.

---

## 30.7. Scaling & Performance Considerations

### 30.7.1. Horizontal Scaling

The API gateway is stateless (aside from DB and any cache), so you can scale replicas based on:

- CPU utilization.  
- Request rate / latency thresholds.  
- Queue depth (if using background workers).

Ensure:

- Sticky sessions are **not** required; JWT‑based auth works across replicas.  
- Rate limit & quota checks are **DB / cache based**, not in‑memory only.

### 30.7.2. Ollama Considerations

- Ollama is GPU/CPU intensive; typically run it on dedicated nodes or hosts.  
- Consider:
  - Separate Ollama deployment from API pods.  
  - Leveraging different model sizes (light vs heavy) via `Conversation.model`.  
  - Caching frequent responses if your use‑case allows it.

### 30.7.3. Safety Limits

Use `ORG_HARD_TOKEN_LIMIT` and `ORG_SOFT_TOKEN_LIMIT` in `orgQuotaGuard` to:

- Prevent abusive or runaway usage.  
- Surface soft warnings in the UI (e.g. “You are near your monthly token limit.”).

Tie these limits into 29.md’s **Org Analytics** for visibility.

---

## 30.8. CI/CD Outline

A minimal CI/CD flow should:

1. **Build & test:**
   - `pnpm install`
   - `pnpm lint`
   - `pnpm typecheck`
   - `pnpm test` (if present)

2. **Build Docker images:**
   - `docker build -f apps/api-gateway/Dockerfile -t your-registry/api-gateway:SHA .`  
   - `docker build -f apps/web/Dockerfile -t your-registry/web:SHA .`

3. **Push images to registry.**

4. **Apply migrations:**
   - `pnpm --filter api-gateway prisma migrate deploy` against the production DB.

5. **Deploy:**
   - Update Docker Compose host or Kubernetes manifests with new image tags.  
   - Use rolling updates (Kubernetes) or a blue/green deployment strategy.

6. **Post‑deploy checks:**
   - Hit `/readyz` and `/metrics` for health.  
   - Verify Grafana dashboards (28.md) and Org Analytics (29.md) show expected behavior.

---

## 30.9. Sanity Checklist

Before considering a deployment “production‑ready”, verify:

- [ ] `DATABASE_URL` and `JWT_SECRET` are set from a **secure secret store** (not hard‑coded).  
- [ ] `/healthz` and `/readyz` are wired into your load balancer / orchestrator.  
- [ ] Prometheus scraping of `/metrics` is configured; Grafana dashboards are live.  
- [ ] Org quota limits are tuned and visible in the Org Analytics UI.  
- [ ] Ollama has required models pulled (`ollama pull llama3`, etc.).  
- [ ] CI/CD pipeline builds, tests, migrates, and deploys without manual steps.

If all points are satisfied, your AI chat system with Ollama is ready for **robust, observable, scalable production deployment**.

---

_End of 30.md – Production Deployment, Configuration & Scaling Guide_
