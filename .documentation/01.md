# 1. System Overview & Vision – Ollama-Powered AI Chat Platform

> **Audience:** AI coding assistants (Cursor, Claude Code, ChatGPT, etc.) and human developers.
>
> **Goal of this file:** Give a complete, crystal-clear, high-level blueprint of the application so that AI agents can implement the entire system without asking the user for clarification.
>
> **Important instruction to AI agents:**
> - Do **not** leave TODOs.
> - Do **not** skip parts because they look optional.
> - When you see choices (A or B), pick **one** and implement it consistently unless later files explicitly say otherwise.
> - If something is ambiguous, choose the **simplest** option that preserves all required features and is consistent with this document.

---

## 1.1. Product Vision

We are building an **industry-leading AI chat platform** that:

- Runs on a **NodeJS** backend with **TypeScript** and a modern React frontend.
- Uses **Ollama** as the primary Large Language Model (LLM) runtime (running locally or on a server).
- Recreates and improves the experience of **ChatGPT**:
  - Multi-conversation chat, conversation history, search.
  - Streaming token-by-token responses.
  - Custom instructions and per-workspace behavior.
  - Tool / function calling ("plugins") with structured inputs/outputs.
  - File uploads + knowledge bases + RAG (Retrieval Augmented Generation).
  - Analytics and usage insights.
- Uses **Material 3** design with **vivid gradients**, **micro-interactions**, and a highly interactive UI.
- Includes a **production-grade observability stack** with **Prometheus + Grafana** and structured logs.

The platform must be:

- **Multi-tenant**: multiple organizations (tenants), each with their own users, settings, and data.
- **Secure**: proper auth, RBAC, basic rate limiting and safety filters.
- **Extensible**: easy to add new tools, models, and integrations.
- **Automatable**: fully spec’d so AI agents can modify and extend it reliably.

---

## 1.2. High-Level Capabilities

At a high level, the system provides:

1. **Core Chat Engine**
   - Chat with multiple Ollama models.
   - Streaming responses via SSE (Server-Sent Events).
   - Chat history persistence, search, and management.
   - Tools / plugins (function calling style).

2. **Knowledge & Files**
   - Upload documents (PDF, txt, markdown, etc.).
   - Automatic chunking + embedding into a vector store.
   - RAG: use selected knowledge bases as context for answers.

3. **Customization**
   - Global custom instructions per user.
   - Workspace/org-level defaults (model, tone, safety level, tools available).
   - Per-conversation settings (model, creativity, attached knowledge bases).

4. **Team & Multi-Tenant Support**
   - Organizations/workspaces with members and roles.
   - Org-level settings, quotas, and analytics.

5. **Tools & Plugins Engine**
   - Tools are defined with metadata and JSON schema for parameters.
   - The LLM can call tools; the backend runs them and feeds results back.
   - Built-in tools (e.g., HTTP fetch, time, simple DB query).

6. **Safety & Moderation**
   - Configurable filters for incoming prompts and outgoing responses.
   - Tenant-configurable aggression / safety level.

7. **Analytics & Observability**
   - Metrics for usage (requests, tokens, latency, errors).
   - Per-tenant dashboards.
   - Logs and traces for debugging.

8. **Modern Frontend UX**
   - Responsive layout (desktop-first, mobile-friendly).
   - Material 3 components, vivid gradients.
   - Framer Motion-based micro-interactions.
   - Code-block aware chat UI (syntax highlighting, copy buttons).

---

## 1.3. Target Users & Personas

Understanding the target users helps ensure the feature set and UX make sense.

### 1.3.1. Individual Developer / Power User

- Wants a **local or self-hosted ChatGPT-like experience** powered by Ollama.
- Needs multi-chat, code-centric answers, and tools like web fetch or file Q&A.
- Often uses AI tools like Cursor, VSCode + extensions, etc.

### 1.3.2. Small Team / Startup

- Needs a **shared AI assistant** across a small team.
- Wants team-specific custom instructions, shared knowledge bases, and internal tools.
- Needs basic analytics: who uses what models, how often, and for what.

### 1.3.3. Larger Organization / Tenant

- Requires **multi-tenant, role-based access** to AI.
- Needs integration with internal systems via tools/plugins.
- Needs visibility into usage, safety policies, and cost estimates.

The system must serve all three personas without customization from the user at install time; configuration should be done through environment variables and admin screens.

---

## 1.4. Non-Functional Requirements

The application must satisfy the following quality attributes:

### 1.4.1. Performance

- Typical chat response latency (end-to-end) should be dominated by LLM generation; overhead from our system should be minimal (~tens of milliseconds).
- Streaming should start as soon as the first chunk from Ollama is available.
- The system must gracefully support **multiple concurrent streaming chats**.

### 1.4.2. Scalability

- Backend services must be stateless where possible, allowing horizontal scaling.
- Long-lived connections (SSE/WebSockets) should be handled by a dedicated API gateway, with proper timeouts.
- Vector / relational databases must be external services that can be scaled separately.

### 1.4.3. Security

- Strong authentication (JWT-based).
- Role-based access control (RBAC) for organizations and admin features.
- Tenant isolation: no user or org can see another org’s data.
- OWASP best practices: secure headers, rate limiting, basic input validation.

### 1.4.4. Reliability & Maintainability

- Health checks and readiness checks for all services.
- Structured logging and metrics for each core path (chat, tools, uploads, etc.).
- Clean and consistent TypeScript types and interfaces across the repo.

### 1.4.5. Observability

- Prometheus metrics for key operations (requests, errors, latency, tokens).
- Logs shipped to Loki (or similar) with correlation IDs.
- At least one Grafana dashboard for API health and one for usage analytics.

### 1.4.6. UX & Design

- Material 3-based UI components for consistency.
- Vivid gradient highlights for primary actions and important sections.
- Smooth animations for chat bubbles, tool invocations, and state changes.

---

## 1.5. High-Level Architecture Overview

### 1.5.1. Monorepo Layout

We use a **monorepo** with separate apps and shared packages. This makes it easy for AI agents to navigate and modify shared code.

Expected structure (this is a conceptual preview; detailed exact structure will be defined in a later `Monorepo` document):

```text
root/
  apps/
    api-gateway/          # NodeJS Fastify-based API and streaming server
    web-app/              # Next.js + React frontend
    worker-jobs/          # Background worker for async jobs (RAG, analytics)

  packages/
    core-types/           # Shared TypeScript types and DTOs
    db/                   # Prisma schema and DB client
    chat-orchestrator/    # Core conversation + LLM interaction logic
    ollama-client/        # Wrapper for Ollama HTTP API
    tools-engine/         # Tools/plugins engine
    telemetry/            # Logging, metrics, tracing utilities

  infra/
    docker/               # Dockerfiles and docker-compose
    k8s/                  # (Optional) Kubernetes manifests

  docs/                   # Tutorial markdown files (this file is 1.md)
```

**Instruction to AI agents:** When you are asked to create folders and files, ensure they align with this high-level structure unless a later markdown file explicitly adjusts it.

### 1.5.2. Logical Services

1. **API Gateway / Backend API (apps/api-gateway)**
   - Handles HTTP requests from frontend.
   - Provides REST endpoints for auth, orgs, conversations, messages, tools, files, etc.
   - Handles SSE streaming for chat.
   - Validates input, enforces RBAC, sends telemetry events.

2. **Chat Orchestrator (packages/chat-orchestrator)**
   - Knows how to construct prompts from:
     - Conversation messages.
     - Custom instructions.
     - Knowledge base snippets.
     - Tool definitions and results.
   - Manages message trimming to fit model context.
   - Invokes Ollama via the Ollama client.

3. **Ollama Client (packages/ollama-client)**
   - Encapsulates HTTP calls to Ollama.
   - Provides streaming and non-streaming APIs.
   - Discovers available models and health.

4. **Database & Persistence (packages/db)**
   - Prisma schema and client.
   - Manages User, Organization, Conversation, Message, Tool, File, Embedding, UsageMetric, ApiKey, etc.

5. **Tools Engine (packages/tools-engine)**
   - Stores tool definitions and metadata.
   - Executes tool functions in a controlled environment.
   - Returns JSON results to the Chat Orchestrator.

6. **Knowledge & RAG**
   - Ingestion pipeline for files into a vector store.
   - Retrieval interface for the Chat Orchestrator.

7. **Telemetry (packages/telemetry)**
   - Prometheus metrics registration.
   - Structured logger.
   - OpenTelemetry instrumentation setup.

8. **Workers (apps/worker-jobs)**
   - Background processing: file ingestion, embeddings, cleanup, analytics aggregation.

---

## 1.6. Core Data Concepts

This section defines the primary domain entities at a conceptual level. The exact schema will be detailed in the `Database Schema` markdown file.

### 1.6.1. User

A person accessing the system.

- Fields (conceptual): id, email, name, password hash, settings (including custom instructions), createdAt, updatedAt.
- Relationships:
  - Belongs to many Organizations via OrgMembership.
  - Has many Conversations.

### 1.6.2. Organization (Tenant)

A workspace or tenant that groups users, conversations, tools, and knowledge bases.

- Fields: id, name, owner, settings, createdAt, updatedAt.
- Relationships:
  - Has many OrgMembers.
  - Has many Conversations.
  - Has many Tools and Knowledge Bases.

### 1.6.3. OrgMember

Connects a User with an Organization and defines their role.

- Fields: id, userId, orgId, role (owner/admin/member), createdAt.

### 1.6.4. Conversation

A thread of messages between the user/org and the AI (and tools).

- Fields: id, title, orgId (optional if personal), userId (creator), model, settings (temperature, etc.), createdAt, updatedAt.
- Relationships:
  - Has many Messages.
  - May be linked to Knowledge Bases.

### 1.6.5. Message

A single unit of content in a conversation.

- Fields: id, conversationId, role (system/user/assistant/tool), content (text), metadata (JSON), createdAt.
- Some messages may represent tool invocation requests or results.

### 1.6.6. Tool

A callable function with a name, description, and JSON schema for parameters.

- Fields: id, orgId (if org-specific), name, description, parametersSchema (JSON), isEnabled, createdAt.
- Implementation: actual runtime code lives in the tools engine.

### 1.6.7. File & Knowledge Base

Documents uploaded by users to be used in RAG.

- File: id, orgId, userId, filename, MIME type, storage key, size, createdAt.
- Chunks/Embeddings: per-file segments stored in a vector index with embedding vectors.

### 1.6.8. Usage Metrics

Aggregated or raw metrics for usage.

- Per request: userId, orgId, model, tokensIn, tokensOut, duration, error flags.
- Aggregated: daily usage per org/model.

---

## 1.7. Stack Choices (Preview)

This section briefly lists the main technologies. Details (versions and installation) will be in a dedicated file.

- **Backend**
  - NodeJS (LTS, 20+ or 22+)
  - TypeScript
  - Fastify (HTTP framework)
  - Prisma (ORM) + PostgreSQL
  - Redis (for queues/cache)
  - OpenTelemetry, Prometheus, Loki, Grafana

- **LLM Runtime**
  - Ollama (local or remote) with support for:
    - `/api/chat` endpoint for streaming chat.
    - `/api/tags` for listing models.

- **Frontend**
  - Next.js + React
  - Material UI (MUI) with Material 3 support
  - Framer Motion (for animations)

- **Vector Store**
  - PostgreSQL + pgvector **or** a dedicated vector DB like Qdrant.
  - For initial implementation, prefer **pgvector** embedded into PostgreSQL.

**Instruction to AI agents:** When later documents specify exact versions or configuration, follow that and ignore any default or implicit versions from your environment.

---

## 1.8. Functional Scope Summary

Here is a concise list of core functions that **must** exist in the final application.

1. **Auth & Users**
   - Register/login with email + password.
   - JWT-based auth.
   - User profile + custom instructions management.

2. **Organizations & Roles**
   - Create org, invite users, assign roles.
   - Org-wide settings: default model, safety level.

3. **Chat & Conversations**
   - Create, rename, delete, and archive conversations.
   - Send user message, stream AI response.
   - Regenerate last response.
   - Attach tools and knowledge bases to a conversation.

4. **Models & Settings**
   - List available Ollama models.
   - Choose model per conversation.
   - Control temperature, max tokens (where supported).

5. **Tools / Plugins**
   - Define tools with JSON schema.
   - Enable/disable tools for a tenant.
   - Execute tools when LLM requests them.

6. **Files & Knowledge**
   - Upload files.
   - View list of files and knowledge bases.
   - Ask questions about a specific file or set of files.

7. **Safety & Moderation**
   - Basic content filters.
   - Configurable safety mode.

8. **Analytics & Usage**
   - Per-org usage dashboards.
   - System health and performance dashboards.

9. **Admin & Maintenance**
   - Basic superadmin view (for self-hosted admin) with system overview.
   - Health checks and logs.

---

## 1.9. Overall Implementation Strategy (For AI Agents)

When implementing this system, AI agents should:

1. **Follow the markdown files in order** (1.md, 2.md, 3.md, etc. or according to the user’s naming convention) unless explicitly instructed otherwise.
2. **Create and modify files only where specified** and keep a consistent project structure.
3. **After implementing each major feature**,:
   - Run `npm test` or `pnpm test` if defined.
   - Run lint and type checks.
   - Fix any failing tests or type errors before proceeding.
4. **Prefer explicit, strongly-typed code** over implicit or magical patterns.
5. **Avoid experimental or unstable frameworks** – stick to stable releases as specified.

---

## 1.10. What Comes Next

Subsequent markdown files (e.g., `2.md`, `3.md`, or named variants like `02_Technology_Stack_and_Prerequisites.md`) will cover:

- Exact technology stack versions and installation instructions.
- Detailed monorepo structure and configuration.
- Precise database schema with Prisma.
- All backend APIs, their request/response shapes, and error handling.
- Tools engine details and sample tools.
- Full frontend architecture and UI implementation details.
- Observability setup and Grafana dashboards.
- Deployment, CI/CD, and maintenance.

**Instruction to AI agents:** Do not attempt to implement everything based only on this file. Use this as a **map** and rely on the next markdown files for step-by-step instructions, code templates, and required configurations.

---

_End of 1.md – System Overview & Vision_
